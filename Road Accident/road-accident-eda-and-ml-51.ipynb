{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'> Road Accident.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-hinbp.nitrocdn.com/bNGDGkJzqqclpSTwXkdOJqLhhVktqhvt/assets/images/optimized/rev-dea4b79/www.kraftlaw.com/wp-content/uploads/2021/10/types-of-car-accidents.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:35:24.506295Z",
     "iopub.status.busy": "2025-01-31T10:35:24.506000Z",
     "iopub.status.idle": "2025-01-31T10:35:24.517537Z",
     "shell.execute_reply": "2025-01-31T10:35:24.516626Z",
     "shell.execute_reply.started": "2025-01-31T10:35:24.506271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:35:42.138870Z",
     "iopub.status.busy": "2025-01-31T10:35:42.138454Z",
     "iopub.status.idle": "2025-01-31T10:35:45.106889Z",
     "shell.execute_reply": "2025-01-31T10:35:45.105662Z",
     "shell.execute_reply.started": "2025-01-31T10:35:42.138835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.ticker import ScalarFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 1. Importing Dataset. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are reading the database using the name \"df\" in the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T12:05:17.026691Z",
     "iopub.status.busy": "2025-01-31T12:05:17.026284Z",
     "iopub.status.idle": "2025-01-31T12:05:17.045407Z",
     "shell.execute_reply": "2025-01-31T12:05:17.042879Z",
     "shell.execute_reply.started": "2025-01-31T12:05:17.026663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/riteshkumar/Downloads/ML projects/Road Accident/accident.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that we have categorical and continuous variables, the dataset contains the following features:\n",
    "- Age: Person Age\n",
    "- Gender: Person Gender\n",
    "- Speed of Impact: Speed of Impact\n",
    "- Hemlet_Used: Helmet used or not\n",
    "- Seatbelt_Used: Seatbelt used or not\n",
    "- Survived: If person survived or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:36:40.588747Z",
     "iopub.status.busy": "2025-01-31T10:36:40.588379Z",
     "iopub.status.idle": "2025-01-31T10:36:40.617996Z",
     "shell.execute_reply": "2025-01-31T10:36:40.616285Z",
     "shell.execute_reply.started": "2025-01-31T10:36:40.588722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:38:28.683202Z",
     "iopub.status.busy": "2025-01-31T10:38:28.682715Z",
     "iopub.status.idle": "2025-01-31T10:38:28.702548Z",
     "shell.execute_reply": "2025-01-31T10:38:28.701067Z",
     "shell.execute_reply.started": "2025-01-31T10:38:28.683170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique_counts = df.nunique()\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:38:37.067661Z",
     "iopub.status.busy": "2025-01-31T10:38:37.067253Z",
     "iopub.status.idle": "2025-01-31T10:38:37.088822Z",
     "shell.execute_reply": "2025-01-31T10:38:37.087753Z",
     "shell.execute_reply.started": "2025-01-31T10:38:37.067631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:38:48.066275Z",
     "iopub.status.busy": "2025-01-31T10:38:48.065908Z",
     "iopub.status.idle": "2025-01-31T10:38:48.073904Z",
     "shell.execute_reply": "2025-01-31T10:38:48.072830Z",
     "shell.execute_reply.started": "2025-01-31T10:38:48.066247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, taking a first look at our continuous data, we can see that we have people ranging from the youngest to the oldest in our dataset, as well as a wide variety in our impact speed variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:38:54.281952Z",
     "iopub.status.busy": "2025-01-31T10:38:54.281614Z",
     "iopub.status.idle": "2025-01-31T10:38:54.305698Z",
     "shell.execute_reply": "2025-01-31T10:38:54.304621Z",
     "shell.execute_reply.started": "2025-01-31T10:38:54.281927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we saw above and can confirm below, we have 3 null values. Since they are very few, I will choose to remove these null values to proceed with the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:39:06.508193Z",
     "iopub.status.busy": "2025-01-31T10:39:06.507773Z",
     "iopub.status.idle": "2025-01-31T10:39:06.517340Z",
     "shell.execute_reply": "2025-01-31T10:39:06.516245Z",
     "shell.execute_reply.started": "2025-01-31T10:39:06.508164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T12:06:09.189751Z",
     "iopub.status.busy": "2025-01-31T12:06:09.189176Z",
     "iopub.status.idle": "2025-01-31T12:06:09.201748Z",
     "shell.execute_reply": "2025-01-31T12:06:09.200351Z",
     "shell.execute_reply.started": "2025-01-31T12:06:09.189712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 2. Data Analysis. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables:\n",
    "#### Looking at our categorical variables, we can start to analyze the population of our dataset and how our data is distributed. Here, we can see that we have a predominantly female population. In our dataset, there is a predominance of people using helmets/seat belts (although we still have about 45% who don’t use them, which is quite alarming). When we look at our target variable, we can see that it is well balanced, with almost 50% of the data in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:41:36.307160Z",
     "iopub.status.busy": "2025-01-31T10:41:36.306718Z",
     "iopub.status.idle": "2025-01-31T10:41:37.244262Z",
     "shell.execute_reply": "2025-01-31T10:41:37.242952Z",
     "shell.execute_reply.started": "2025-01-31T10:41:36.307128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_percentage(ax, total):\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height() / total)\n",
    "        x = p.get_x() + p.get_width() / 2\n",
    "        y = p.get_height()\n",
    "        ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=15)\n",
    "\n",
    "def add_mean_line(ax, total):\n",
    "    heights = [p.get_height() for p in ax.patches]\n",
    "    mean_count = sum(heights) / len(heights)  \n",
    "\n",
    "    ax.axhline(mean_count, color='red', linestyle='--', label=f'Mean: {mean_count:.1f}')\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "def add_count(ax):\n",
    "    for p in ax.patches:\n",
    "        count = int(p.get_height())\n",
    "        x = p.get_x() + p.get_width() / 2\n",
    "        y = p.get_height() / 2  \n",
    "        ax.annotate(f'{count}', (x, y), ha='center', va='center', fontsize=15)\n",
    "\n",
    "plt.figure(figsize=(25, 13))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.gca().set_title('Variable Gender')\n",
    "ax1 = sns.countplot(x='Gender', palette='Set2', data=df)\n",
    "add_percentage(ax1, len(df['Gender']))\n",
    "add_mean_line(ax1, len(df['Gender']))\n",
    "add_count(ax1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.gca().set_title('Variable Helmet_Used')\n",
    "ax2 = sns.countplot(x='Helmet_Used', palette='Set2', data=df)\n",
    "add_percentage(ax2, len(df['Helmet_Used']))\n",
    "add_mean_line(ax2, len(df['Helmet_Used']))\n",
    "add_count(ax2)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.gca().set_title('Variable Seatbelt_Used')\n",
    "ax3 = sns.countplot(x='Seatbelt_Used',  palette='Set2', data=df)\n",
    "add_percentage(ax3, len(df['Seatbelt_Used']))\n",
    "add_mean_line(ax3, len(df['Seatbelt_Used']))\n",
    "add_count(ax3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.gca().set_title('Variable Survived')\n",
    "ax4 = sns.countplot(x='Survived', palette='Set2', data=df)\n",
    "add_percentage(ax4, len(df['Survived']))\n",
    "add_mean_line(ax4, len(df['Survived']))\n",
    "add_count(ax4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Variables:\n",
    "#### When we look at our continuous variables, we can confirm that our dataset is well distributed, despite being a small dataset. When we look at the first variable, which is age, we can see that the data is well distributed between 18 and 70 years, with some variations at certain ages and a peak around 45 years, but it is still a very well-distributed variable.\n",
    "\n",
    "#### Looking at the speed of impact variable, we can see a smaller distribution, but it still exists. We can observe that the peak of accidents is around 120, which is the highest speed, but we can also verify that we have examples of accidents at all speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:42:38.067080Z",
     "iopub.status.busy": "2025-01-31T10:42:38.066583Z",
     "iopub.status.idle": "2025-01-31T10:42:38.847957Z",
     "shell.execute_reply": "2025-01-31T10:42:38.846806Z",
     "shell.execute_reply.started": "2025-01-31T10:42:38.067042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "plt.figure(figsize=(25, 13))\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "def create_histogram(data, subplot_position):\n",
    "    ax = plt.subplot(2, 1, subplot_position) \n",
    "    sns.histplot(data, kde=False, ax=ax)\n",
    "    \n",
    "    # Definir formato do eixo Y\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.ticklabel_format(useOffset=False) \n",
    "\n",
    "# Criar histogramas\n",
    "create_histogram(df['Age'], 1)\n",
    "create_histogram(df['Speed_of_Impact'], 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the boxplots, we can verify that we don't have any outliers (which is great, as this means there is no need for treatment, given that we don’t have a large dataset). Looking at our age variable, we can confirm what we saw in our histogram. Our average age is 43 years, with the youngest person being 18 years old and the oldest being 69. We can also see that 50% of our data is between 18 and 44 years, and the other 50% is between 44 and 69 years.\n",
    "\n",
    "#### Looking at our other continuous variable, which is the accident speed, we can see that the average is 70 km/h, but we have accidents at all speeds. We can also see that 50% of our accidents are below 71 km/h, which is a number that surprised me a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:47:07.509588Z",
     "iopub.status.busy": "2025-01-31T10:47:07.509124Z",
     "iopub.status.idle": "2025-01-31T10:47:08.325045Z",
     "shell.execute_reply": "2025-01-31T10:47:08.323892Z",
     "shell.execute_reply.started": "2025-01-31T10:47:07.509553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))  \n",
    "\n",
    "data_1 = df[\"Age\"]\n",
    "quartiles_1 = np.percentile(data_1, [25, 50, 75])\n",
    "min_1, max_1, mean_1 = data_1.min(), data_1.max(), data_1.mean()\n",
    "\n",
    "axes[0].set_title(\"Boxplot Age\", fontdict={'fontsize': 20}) \n",
    "sns.boxplot(x=data_1, ax=axes[0], boxprops=dict(color='#4C72B0'), medianprops=dict(color='#56B4E9'))\n",
    "\n",
    "axes[0].text(quartiles_1[0], 0.80, f\"Q1: {quartiles_1[0]:.2f}\", ha='center', va='bottom', fontsize=12, color='#4C72B0')\n",
    "axes[0].text(quartiles_1[1], 0.80, f\"Median: {quartiles_1[1]:.2f}\", ha='center', va='bottom', fontsize=12, color='#56B4E9')\n",
    "axes[0].text(quartiles_1[2], 0.80, f\"Q3: {quartiles_1[2]:.2f}\", ha='center', va='bottom', fontsize=12, color='#4C72B0')\n",
    "axes[0].text(min_1, 1.1, f\"Min: {min_1:.2f}\", ha='center', va='bottom', fontsize=12, color='green')\n",
    "axes[0].text(max_1, 1.1, f\"Max: {max_1:.2f}\", ha='center', va='bottom', fontsize=12, color='red')\n",
    "axes[0].text(mean_1, 1.1, f\"Mean: {mean_1:.2f}\", ha='center', va='bottom', fontsize=12, color='purple')\n",
    "\n",
    "data_2 = df[\"Speed_of_Impact\"]\n",
    "quartiles_2 = np.percentile(data_2, [25, 50, 75])\n",
    "min_2, max_2, mean_2 = data_2.min(), data_2.max(), data_2.mean()\n",
    "\n",
    "axes[1].set_title(\"Boxplot Speed_of_Impact\", fontdict={'fontsize': 20})  \n",
    "sns.boxplot(x=data_2, ax=axes[1], boxprops=dict(color='#4C72B0'), medianprops=dict(color='#56B4E9'))\n",
    "\n",
    "# Add statistics annotations for Speed_of_Impact\n",
    "axes[1].text(quartiles_2[0], 0.80, f\"Q1: {quartiles_2[0]:.2f}\", ha='center', va='bottom', fontsize=12, color='#4C72B0')\n",
    "axes[1].text(quartiles_2[1], 0.80, f\"Median: {quartiles_2[1]:.2f}\", ha='center', va='bottom', fontsize=12, color='#56B4E9')\n",
    "axes[1].text(quartiles_2[2], 0.80, f\"Q3: {quartiles_2[2]:.2f}\", ha='center', va='bottom', fontsize=12, color='#4C72B0')\n",
    "axes[1].text(min_2, 1.1, f\"Min: {min_2:.2f}\", ha='center', va='bottom', fontsize=12, color='green')\n",
    "axes[1].text(max_2, 1.1, f\"Max: {max_2:.2f}\", ha='center', va='bottom', fontsize=12, color='red')\n",
    "axes[1].text(mean_2, 1.1, f\"Mean: {mean_2:.2f}\", ha='center', va='bottom', fontsize=12, color='purple')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s start our bivariate analysis by comparing our categorical variables with our target variable (Survived). When we look at the first variable, Gender, we can observe that there is a higher probability of a female person dying than a male person (which I thought would be the complete opposite). However, we also don't have information on whether the person was driving or just in the car, but what we can confirm is that women are more likely to die in an accident, at least with this dataset.\n",
    "\n",
    "#### One thing that really surprised me when looking at the continuous variables was the 'Helmet_Used' variable, where people who didn’t use a helmet had a higher chance of surviving than those who did. As I mentioned before, we don’t have much information to complement this, but it is an interesting finding.\n",
    "\n",
    "#### Looking at the last variable, which is about seat belt usage, we see a behavior we could expect: when the person doesn’t use a seat belt, they have a higher probability of not surviving, which is the opposite when they do use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T12:05:28.928912Z",
     "iopub.status.busy": "2025-01-31T12:05:28.928303Z",
     "iopub.status.idle": "2025-01-31T12:05:30.199306Z",
     "shell.execute_reply": "2025-01-31T12:05:30.197349Z",
     "shell.execute_reply.started": "2025-01-31T12:05:28.928793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_percentage_and_count_by_group(ax, data, x, hue):\n",
    "    counts = data.groupby([x, hue]).size().unstack(fill_value=0)\n",
    "    \n",
    "    percentages = counts.apply(lambda c: c / c.sum() * 100, axis=1)\n",
    "    \n",
    "    hue_order = ax.legend_.get_texts()\n",
    "    hue_labels = [t.get_text() for t in hue_order]\n",
    "    \n",
    "    for i, c in enumerate(ax.containers):\n",
    "        labels = []\n",
    "        for j, v in enumerate(c):\n",
    "            height = v.get_height()\n",
    "            if height > 0:\n",
    "                try:\n",
    "                    percentage = percentages.iloc[j, percentages.columns.get_loc(hue_labels[i])]\n",
    "                except KeyError:\n",
    "                    percentage = percentages.iloc[j, i]\n",
    "                \n",
    "                count = int(v.get_height())  \n",
    "                labels.append(f'{count}\\n({percentage:.1f}%)')\n",
    "            else:\n",
    "                labels.append('')\n",
    "        \n",
    "        ax.bar_label(c, labels=labels, label_type='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.set_ylim(0, ax.get_ylim()[1] * 1.1)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.suptitle(\"Analysis Of Variable target (Survived)\", fontweight=\"bold\", fontsize=20)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.gca().set_title('Variable Gender')\n",
    "ax1 = sns.countplot(x='Gender', hue='Survived', palette='Set2', data=df)\n",
    "add_percentage_and_count_by_group(ax1, df, 'Gender', 'Survived')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.gca().set_title('Variable Helmet_Used')\n",
    "ax2 = sns.countplot(x='Helmet_Used', hue='Survived', palette='Set2', data=df)\n",
    "add_percentage_and_count_by_group(ax2, df, 'Helmet_Used', 'Survived')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.gca().set_title('Variable Seatbelt_Used')\n",
    "ax3 = sns.countplot(x='Seatbelt_Used', hue='Survived', palette='Set2', data=df)\n",
    "add_percentage_and_count_by_group(ax3, df, 'Seatbelt_Used', 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving on to our continuous variables, we can also draw some insights. Although we have data on people of all ages who did not survive, when we look at our age variable, we can see that our Q1 (25% of our data) behaves differently. The average age for people who did not die is 32, compared to 28, which means that, in this case, there is a lower tendency for fatal accidents above 30 years old, which makes a lot of sense.\n",
    "\n",
    "#### When we look at the impact speed variable, we don’t have a well-defined pattern, which is quite strange because we usually tend to think that accidents involving higher speeds would have a higher chance of being fatal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:51:28.541897Z",
     "iopub.status.busy": "2025-01-31T10:51:28.541475Z",
     "iopub.status.idle": "2025-01-31T10:51:30.500762Z",
     "shell.execute_reply": "2025-01-31T10:51:30.499556Z",
     "shell.execute_reply.started": "2025-01-31T10:51:28.541869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_quartile_labels_by_hue(ax, x, hue, data):\n",
    "    # Calcular os quartis para cada grupo de 'hue' (Preference)\n",
    "    quartiles = data.groupby(hue)[x].quantile([0.25, 0.5, 0.75]).unstack()\n",
    "\n",
    "    # Obter as posições dos boxplots no eixo x\n",
    "    positions = ax.get_xticks()  # Posições dos boxplots\n",
    "    hue_values = data[hue].unique()  # Valores únicos de 'hue' (Preference)\n",
    "\n",
    "    # Inverter a ordem das classes de hue (classe 1 primeiro, classe 0 depois)\n",
    "    hue_values = hue_values[::-1]\n",
    "\n",
    "    # Certifique-se de que os valores de 'hue' correspondem corretamente aos boxplots\n",
    "    for i, pos in enumerate(positions):\n",
    "        # Pegue a classe de 'hue' correspondente à posição do boxplot\n",
    "        preference = hue_values[i]\n",
    "        \n",
    "        # Obter os quartis (Q1, Q2, Q3) para esse grupo\n",
    "        Q1 = quartiles.loc[preference, 0.25]\n",
    "        Q2 = quartiles.loc[preference, 0.5]  # Mediana\n",
    "        Q3 = quartiles.loc[preference, 0.75]\n",
    "\n",
    "        # Adicionar as anotações para os quartis (Q1, Mediana e Q3) na posição correta\n",
    "        ax.text(pos, Q1, f'Q1: {Q1:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax.text(pos, Q2, f'Median: {Q2:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=10, color='green')\n",
    "        ax.text(pos, Q3, f'Q3: {Q3:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=10, color='red')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 13))\n",
    "plt.suptitle(\"Analysis Of Variable target (Survived)\", fontweight=\"bold\", fontsize=20)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "ax1 = sns.boxplot(x='Survived', y='Age', data=df, hue=\"Survived\", palette='Set3', hue_order=[1, 0])\n",
    "plt.title('Boxplot of Age vs Survived')\n",
    "add_quartile_labels_by_hue(ax1, 'Age', 'Survived', df)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "ax2 = sns.violinplot(x='Survived', y='Age', data=df, hue=\"Survived\", palette='Set3', hue_order=[1, 0])\n",
    "plt.title('Violin plot of Age vs Survived')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "ax3 = sns.stripplot(x='Survived', y='Age', data=df, hue=\"Survived\", palette='Set3', hue_order=[1, 0])\n",
    "plt.title('stripplot of Age vs Survived')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "ax4 = sns.boxplot(x='Survived', y='Speed_of_Impact', data=df, hue=\"Survived\", palette='Set3', hue_order=[1, 0])\n",
    "plt.title('Boxplot of Speed_of_Impact vs Survived')\n",
    "add_quartile_labels_by_hue(ax4, 'Speed_of_Impact', 'Survived', df)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "ax5 = sns.violinplot(x='Survived', y='Speed_of_Impact', data=df, hue=\"Survived\", palette='Set3', hue_order=[1, 0])\n",
    "plt.title('Violin plot of Speed_of_Impact vs Survived')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "ax6 = sns.stripplot(x='Survived', y='Speed_of_Impact', data=df, hue=\"Survived\", palette='Set3', hue_order=[1, 0])\n",
    "plt.title('stripplot of Speed_of_Impact vs Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to analyze the age of our users with the impact speed to check if younger people are more likely to drive faster, we couldn’t find any pattern between age and impact speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:50:02.631169Z",
     "iopub.status.busy": "2025-01-31T11:50:02.630673Z",
     "iopub.status.idle": "2025-01-31T11:50:03.061504Z",
     "shell.execute_reply": "2025-01-31T11:50:03.060281Z",
     "shell.execute_reply.started": "2025-01-31T11:50:02.631132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "axes = []\n",
    "\n",
    "axes.append(plt.subplot(1, 1, 1))\n",
    "sns.scatterplot(x='Age', y='Speed_of_Impact', data=df)\n",
    "plt.title('Scatterplot of Age vs Speed_of_Impact')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.ticklabel_format(style='plain', axis='both')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing our age variable with helmet/seat belt usage, although there is a slight difference, we couldn’t find much difference between people who use them and those who don’t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T12:03:12.665241Z",
     "iopub.status.busy": "2025-01-31T12:03:12.664804Z",
     "iopub.status.idle": "2025-01-31T12:03:15.217163Z",
     "shell.execute_reply": "2025-01-31T12:03:15.215893Z",
     "shell.execute_reply.started": "2025-01-31T12:03:12.665213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_quartile_labels_by_hue(ax, x, hue, data):\n",
    "    quartiles = data.groupby(hue)[x].quantile([0.25, 0.5, 0.75]).unstack()\n",
    "    positions = ax.get_xticks()\n",
    "    hue_values = data[hue].unique()\n",
    "    # Remover esta linha: hue_values = hue_values[::-1]\n",
    "    \n",
    "    for i, pos in enumerate(positions):\n",
    "        preference = hue_values[i]\n",
    "        Q1 = quartiles.loc[preference, 0.25]\n",
    "        Q2 = quartiles.loc[preference, 0.5]\n",
    "        Q3 = quartiles.loc[preference, 0.75]\n",
    "        ax.text(pos, Q1, f'Q1: {Q1:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax.text(pos, Q2, f'Median: {Q2:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=10, color='green')\n",
    "        ax.text(pos, Q3, f'Q3: {Q3:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=10, color='red')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 13))\n",
    "plt.suptitle(\"Other Analysis\", fontweight=\"bold\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "ax1 = sns.boxplot(x='Helmet_Used', y='Age', data=df, hue=\"Helmet_Used\", palette='Set3')\n",
    "plt.title('Boxplot of Age vs Helmet_Used')\n",
    "add_quartile_labels_by_hue(ax1, 'Age', 'Helmet_Used', df)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "ax2 = sns.violinplot(x='Helmet_Used', y='Age', data=df, hue=\"Helmet_Used\", palette='Set3')\n",
    "plt.title('Violin plot of Age vs Helmet_Used')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "ax3 = sns.stripplot(x='Helmet_Used', y='Age', data=df, hue=\"Helmet_Used\", palette='Set3')\n",
    "plt.title('stripplot of Age vs Helmet_Used')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "ax4 = sns.boxplot(x='Seatbelt_Used', y='Age', data=df, hue=\"Seatbelt_Used\", palette='Set3')\n",
    "plt.title('Boxplot of Age vs Seatbelt_Used')\n",
    "add_quartile_labels_by_hue(ax4, 'Age', 'Seatbelt_Used', df)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "ax5 = sns.violinplot(x='Seatbelt_Used', y='Age', data=df, hue=\"Seatbelt_Used\", palette='Set3')\n",
    "plt.title('Violin plot of Age vs Seatbelt_Used')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "ax6 = sns.stripplot(x='Seatbelt_Used', y='Age', data=df, hue=\"Seatbelt_Used\", palette='Set3')\n",
    "plt.title('stripplot of Age vs Seatbelt_Used')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, in our final analysis, we got a very interesting result. We found that higher-speed impacts are much more likely to involve females than males, with a significant difference. When we look at our Q4 and compare, we can see that the top 25% of female accidents are between 104 and 120 km/h, whereas for males, this range is from 86.50 to 120 km/h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T12:06:21.910598Z",
     "iopub.status.busy": "2025-01-31T12:06:21.910189Z",
     "iopub.status.idle": "2025-01-31T12:06:22.946487Z",
     "shell.execute_reply": "2025-01-31T12:06:22.945343Z",
     "shell.execute_reply.started": "2025-01-31T12:06:21.910569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "plt.suptitle(\"Other Analysis\", fontweight=\"bold\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "ax1 = sns.boxplot(x='Gender', y='Speed_of_Impact', data=df, hue=\"Gender\", palette='Set3')\n",
    "plt.title('Boxplot of Speed_of_Impact vs Gender')\n",
    "add_quartile_labels_by_hue(ax1, 'Speed_of_Impact', 'Gender', df)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "ax2 = sns.violinplot(x='Gender', y='Speed_of_Impact', data=df, hue=\"Gender\", palette='Set3')\n",
    "plt.title('Violin plot of Speed_of_Impact vs Gender')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "ax3 = sns.stripplot(x='Gender', y='Speed_of_Impact', data=df, hue=\"Gender\", palette='Set3')\n",
    "plt.title('stripplot of Speed_of_Impact vs Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 3. Model Building. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoder\n",
    "- Here we are going to use the LabelEncoder to transform our categorical variables into numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:53:28.687234Z",
     "iopub.status.busy": "2025-01-31T10:53:28.686871Z",
     "iopub.status.idle": "2025-01-31T10:53:28.695920Z",
     "shell.execute_reply": "2025-01-31T10:53:28.694602Z",
     "shell.execute_reply.started": "2025-01-31T10:53:28.687209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_encoder_Gender = LabelEncoder()\n",
    "label_encoder_Helmet_Used = LabelEncoder()\n",
    "label_encoder_Seatbelt_Used = LabelEncoder()\n",
    "\n",
    "df['Gender'] = label_encoder_Gender.fit_transform(df['Gender'])\n",
    "df['Helmet_Used'] = label_encoder_Helmet_Used.fit_transform(df['Helmet_Used'])\n",
    "df['Seatbelt_Used'] = label_encoder_Seatbelt_Used.fit_transform(df['Seatbelt_Used'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating into features variables and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:53:55.185792Z",
     "iopub.status.busy": "2025-01-31T10:53:55.185384Z",
     "iopub.status.idle": "2025-01-31T10:53:55.192529Z",
     "shell.execute_reply": "2025-01-31T10:53:55.191267Z",
     "shell.execute_reply.started": "2025-01-31T10:53:55.185758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df.drop('Survived', axis = 1)\n",
    "X = X.values\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler\n",
    "- Here we will use StandardScaler to put our data in the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:54:40.481387Z",
     "iopub.status.busy": "2025-01-31T10:54:40.481026Z",
     "iopub.status.idle": "2025-01-31T10:54:40.489370Z",
     "shell.execute_reply": "2025-01-31T10:54:40.487810Z",
     "shell.execute_reply.started": "2025-01-31T10:54:40.481359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_standard = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Data into Train e Test, here we will use 30% of our data to test the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:55:31.318083Z",
     "iopub.status.busy": "2025-01-31T10:55:31.317471Z",
     "iopub.status.idle": "2025-01-31T10:55:31.326124Z",
     "shell.execute_reply": "2025-01-31T10:55:31.324654Z",
     "shell.execute_reply.started": "2025-01-31T10:55:31.318045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X_standard, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "- Running Gaussian Model.\n",
    "- Here we will use the Naive Bayes Model, we will test Gaussian model, using our Normal Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In our first machine learning model, we obtained a very poor result. The model was terrible at predicting both the negative outcome (death) and the positive outcome (survival), achieving only 47% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:55:57.592262Z",
     "iopub.status.busy": "2025-01-31T10:55:57.591903Z",
     "iopub.status.idle": "2025-01-31T10:55:57.748698Z",
     "shell.execute_reply": "2025-01-31T10:55:57.747558Z",
     "shell.execute_reply.started": "2025-01-31T10:55:57.592235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "previsoes = naive_bayes.predict(X_test)\n",
    "\n",
    "cm = ConfusionMatrix(naive_bayes)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:56:11.399606Z",
     "iopub.status.busy": "2025-01-31T10:56:11.399154Z",
     "iopub.status.idle": "2025-01-31T10:56:11.416714Z",
     "shell.execute_reply": "2025-01-31T10:56:11.415214Z",
     "shell.execute_reply.started": "2025-01-31T10:56:11.399563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classification_naive_gaussian = (classification_report(y_test, previsoes))\n",
    "print(classification_naive_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:56:24.794860Z",
     "iopub.status.busy": "2025-01-31T10:56:24.794470Z",
     "iopub.status.idle": "2025-01-31T10:56:24.799643Z",
     "shell.execute_reply": "2025-01-31T10:56:24.798494Z",
     "shell.execute_reply.started": "2025-01-31T10:56:24.794834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "score_naive_gaussian = 0.4745762711864407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "- Here we will use the Decision Tree Model, we will test Entropy and Gini calculations.\n",
    "- Here we are applying GridSearch to check which are the best metrics to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:57:15.756361Z",
     "iopub.status.busy": "2025-01-31T10:57:15.755945Z",
     "iopub.status.idle": "2025-01-31T10:57:18.081268Z",
     "shell.execute_reply": "2025-01-31T10:57:18.080166Z",
     "shell.execute_reply.started": "2025-01-31T10:57:15.756332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [3, 4, 5, 6, 7, 9, 11],\n",
    "              'min_samples_split': [2, 3, 4, 5, 6, 7],\n",
    "              'criterion': ['entropy', 'gini']\n",
    "             }\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "gridDecisionTree = RandomizedSearchCV(model, parameters, cv = 3, n_jobs = -1)\n",
    "gridDecisionTree.fit(X_train, y_train)\n",
    "\n",
    "print('Mín Split: ', gridDecisionTree.best_estimator_.min_samples_split)\n",
    "print('Max Nvl: ', gridDecisionTree.best_estimator_.max_depth)\n",
    "print('Algorithm: ', gridDecisionTree.best_estimator_.criterion)\n",
    "print('Score: ', gridDecisionTree.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can say that it was another terrible result, despite a slight improvement in accuracy. Still, it’s a very poor outcome. The probability of getting it right is the same as flipping a coin and checking if it lands on heads or tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:57:41.219346Z",
     "iopub.status.busy": "2025-01-31T10:57:41.218902Z",
     "iopub.status.idle": "2025-01-31T10:57:41.371173Z",
     "shell.execute_reply": "2025-01-31T10:57:41.370076Z",
     "shell.execute_reply.started": "2025-01-31T10:57:41.219314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 3, max_depth= 5, random_state=0)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "previsoes = decision_tree.predict(X_test)\n",
    "\n",
    "cm = ConfusionMatrix(decision_tree)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:58:04.900325Z",
     "iopub.status.busy": "2025-01-31T10:58:04.899910Z",
     "iopub.status.idle": "2025-01-31T10:58:04.916030Z",
     "shell.execute_reply": "2025-01-31T10:58:04.914715Z",
     "shell.execute_reply.started": "2025-01-31T10:58:04.900294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classification_decision = (classification_report(y_test, previsoes))\n",
    "print(classification_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:58:23.095963Z",
     "iopub.status.busy": "2025-01-31T10:58:23.095466Z",
     "iopub.status.idle": "2025-01-31T10:58:23.101000Z",
     "shell.execute_reply": "2025-01-31T10:58:23.099578Z",
     "shell.execute_reply.started": "2025-01-31T10:58:23.095933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "score_tree = 0.5084745762711864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:04:53.288077Z",
     "iopub.status.busy": "2025-01-31T11:04:53.287673Z",
     "iopub.status.idle": "2025-01-31T11:04:53.301923Z",
     "shell.execute_reply": "2025-01-31T11:04:53.300446Z",
     "shell.execute_reply.started": "2025-01-31T11:04:53.288037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns = df.drop('Survived', axis = 1).columns\n",
    "feature_imp = pd.Series(decision_tree.feature_importances_, index = columns).sort_values(ascending = False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest\n",
    "- Here we will use the Random Forest Model, we will test Entropy and Gini calculations.\n",
    "- Applying GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:58:55.710657Z",
     "iopub.status.busy": "2025-01-31T10:58:55.710238Z",
     "iopub.status.idle": "2025-01-31T10:59:00.545864Z",
     "shell.execute_reply": "2025-01-31T10:59:00.544464Z",
     "shell.execute_reply.started": "2025-01-31T10:58:55.710628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "parameters = {'max_depth': [3, 4, 5, 6, 7, 9, 11],\n",
    "              'min_samples_split': [2, 3, 4, 5, 6, 7],\n",
    "              'criterion': ['entropy', 'gini']\n",
    "             }\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "gridRandomForest = RandomizedSearchCV(model, parameters, cv = 5, n_jobs = -1)\n",
    "gridRandomForest.fit(X_train, y_train)\n",
    "\n",
    "print('Algorithm: ', gridRandomForest.best_estimator_.criterion)\n",
    "print('Score: ', gridRandomForest.best_score_)\n",
    "print('Mín Split: ', gridRandomForest.best_estimator_.min_samples_split)\n",
    "print('Max Nvl: ', gridRandomForest.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the Random Forest model, we obtained an even worse result than the other two, with the same issue of being unable to correctly predict both outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:59:27.781292Z",
     "iopub.status.busy": "2025-01-31T10:59:27.780930Z",
     "iopub.status.idle": "2025-01-31T10:59:28.092100Z",
     "shell.execute_reply": "2025-01-31T10:59:28.090629Z",
     "shell.execute_reply.started": "2025-01-31T10:59:27.781267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators = 100, min_samples_split = 2, max_depth= 4,  criterion = 'entropy', random_state = 0)\n",
    "random_forest.fit(X_train, y_train)\n",
    "previsoes = random_forest.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, previsoes)\n",
    "confusion = confusion_matrix(y_test, previsoes)\n",
    "classification_report_result = classification_report(y_test, previsoes)\n",
    "\n",
    "cm = ConfusionMatrix(random_forest)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:59:43.729174Z",
     "iopub.status.busy": "2025-01-31T10:59:43.728724Z",
     "iopub.status.idle": "2025-01-31T10:59:43.747039Z",
     "shell.execute_reply": "2025-01-31T10:59:43.745598Z",
     "shell.execute_reply.started": "2025-01-31T10:59:43.729142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classification_random = (classification_report(y_test, previsoes))\n",
    "print(classification_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T10:59:58.045192Z",
     "iopub.status.busy": "2025-01-31T10:59:58.044739Z",
     "iopub.status.idle": "2025-01-31T10:59:58.051057Z",
     "shell.execute_reply": "2025-01-31T10:59:58.049698Z",
     "shell.execute_reply.started": "2025-01-31T10:59:58.045154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "score_random = 0.4406779661016949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:05:11.086985Z",
     "iopub.status.busy": "2025-01-31T11:05:11.086607Z",
     "iopub.status.idle": "2025-01-31T11:05:11.109782Z",
     "shell.execute_reply": "2025-01-31T11:05:11.107006Z",
     "shell.execute_reply.started": "2025-01-31T11:05:11.086960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_imp_random = pd.Series(random_forest.feature_importances_, index = columns).sort_values(ascending = False)\n",
    "feature_imp_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Neighbors\n",
    "- Here we will use the K-Neighbors Model, we will use the GridSearch Model to figure out the best metrics to use in this model.\n",
    "- Here we will use the GridSearch to figure out the best metrics to use in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:00:28.909533Z",
     "iopub.status.busy": "2025-01-31T11:00:28.909135Z",
     "iopub.status.idle": "2025-01-31T11:00:29.001678Z",
     "shell.execute_reply": "2025-01-31T11:00:29.000488Z",
     "shell.execute_reply.started": "2025-01-31T11:00:28.909488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "k_list = list(range(1,10))\n",
    "k_values = dict(n_neighbors = k_list)\n",
    "grid = GridSearchCV(knn, k_values, cv = 2, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running K-Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I won’t go too deep to avoid being repetitive, but we obtained the same pattern as the other models: a terrible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:00:48.685440Z",
     "iopub.status.busy": "2025-01-31T11:00:48.685027Z",
     "iopub.status.idle": "2025-01-31T11:00:48.863814Z",
     "shell.execute_reply": "2025-01-31T11:00:48.862634Z",
     "shell.execute_reply.started": "2025-01-31T11:00:48.685412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 9, metric = 'minkowski', p = 1)\n",
    "knn.fit(X_train, y_train)\n",
    "previsoes = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, previsoes)\n",
    "confusion = confusion_matrix(y_test, previsoes)\n",
    "classification_report_result = classification_report(y_test, previsoes)\n",
    "\n",
    "cm = ConfusionMatrix(knn)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:01:36.330545Z",
     "iopub.status.busy": "2025-01-31T11:01:36.330070Z",
     "iopub.status.idle": "2025-01-31T11:01:36.346932Z",
     "shell.execute_reply": "2025-01-31T11:01:36.345528Z",
     "shell.execute_reply.started": "2025-01-31T11:01:36.330484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classification_knn = (classification_report(y_test, previsoes))\n",
    "print(classification_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:01:56.703160Z",
     "iopub.status.busy": "2025-01-31T11:01:56.702713Z",
     "iopub.status.idle": "2025-01-31T11:01:56.707576Z",
     "shell.execute_reply": "2025-01-31T11:01:56.706737Z",
     "shell.execute_reply.started": "2025-01-31T11:01:56.703131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "score_knn = 0.4915254237288136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "- Here we will use the Linear Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I won’t go too deep to avoid being repetitive, but we obtained the same pattern as the other models: a terrible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:02:24.451236Z",
     "iopub.status.busy": "2025-01-31T11:02:24.450893Z",
     "iopub.status.idle": "2025-01-31T11:02:24.616043Z",
     "shell.execute_reply": "2025-01-31T11:02:24.615055Z",
     "shell.execute_reply.started": "2025-01-31T11:02:24.451211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(random_state = 1, max_iter=10000)\n",
    "logistic.fit(X_train, y_train)\n",
    "previsoes = logistic.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, previsoes)\n",
    "confusion = confusion_matrix(y_test, previsoes)\n",
    "classification_report_result = classification_report(y_test, previsoes)\n",
    "\n",
    "cm = ConfusionMatrix(logistic)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:02:41.790538Z",
     "iopub.status.busy": "2025-01-31T11:02:41.790119Z",
     "iopub.status.idle": "2025-01-31T11:02:41.810330Z",
     "shell.execute_reply": "2025-01-31T11:02:41.808596Z",
     "shell.execute_reply.started": "2025-01-31T11:02:41.790482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logistic_normal = (classification_report(y_test, previsoes))\n",
    "print(logistic_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:03:01.249874Z",
     "iopub.status.busy": "2025-01-31T11:03:01.249402Z",
     "iopub.status.idle": "2025-01-31T11:03:01.255569Z",
     "shell.execute_reply": "2025-01-31T11:03:01.253975Z",
     "shell.execute_reply.started": "2025-01-31T11:03:01.249839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logistic_normal = 0.5084745762711864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "- Here we will use the AdaBoost Model, we will use the GridSearch Model to figure out the best metrics to use in this model.\n",
    "- Applying GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:06:54.083383Z",
     "iopub.status.busy": "2025-01-31T11:06:54.083011Z",
     "iopub.status.idle": "2025-01-31T11:07:03.391271Z",
     "shell.execute_reply": "2025-01-31T11:07:03.390169Z",
     "shell.execute_reply.started": "2025-01-31T11:06:54.083356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "parameters = {'learning_rate': [0.01, 0.02, 0.05, 0.07, 0.09, 0.1, 0.3, 0.001, 0.005],\n",
    "              'n_estimators': [300, 500]\n",
    "             }\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "gridAdaBoost = RandomizedSearchCV(model, parameters, cv = 2, n_jobs = -1)\n",
    "gridAdaBoost.fit(X_train, y_train)\n",
    "\n",
    "print('Learning Rate: ', gridAdaBoost.best_estimator_.learning_rate)\n",
    "print('Score: ', gridAdaBoost.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I won’t go too deep to avoid being repetitive, but we obtained the same pattern as the other models: a terrible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:07:18.791815Z",
     "iopub.status.busy": "2025-01-31T11:07:18.791384Z",
     "iopub.status.idle": "2025-01-31T11:07:19.893249Z",
     "shell.execute_reply": "2025-01-31T11:07:19.892240Z",
     "shell.execute_reply.started": "2025-01-31T11:07:18.791783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ada_boost = AdaBoostClassifier(n_estimators = 500, learning_rate =  0.09, random_state = 0)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "previsoes = ada_boost.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, previsoes)\n",
    "confusion = confusion_matrix(y_test, previsoes)\n",
    "classification_report_result = classification_report(y_test, previsoes)\n",
    "\n",
    "cm = ConfusionMatrix(ada_boost)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:07:32.787726Z",
     "iopub.status.busy": "2025-01-31T11:07:32.787259Z",
     "iopub.status.idle": "2025-01-31T11:07:32.806280Z",
     "shell.execute_reply": "2025-01-31T11:07:32.804859Z",
     "shell.execute_reply.started": "2025-01-31T11:07:32.787695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classification_ada_scaler = (classification_report(y_test, previsoes))\n",
    "print(classification_ada_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T11:07:55.415923Z",
     "iopub.status.busy": "2025-01-31T11:07:55.415439Z",
     "iopub.status.idle": "2025-01-31T11:07:55.421665Z",
     "shell.execute_reply": "2025-01-31T11:07:55.420002Z",
     "shell.execute_reply.started": "2025-01-31T11:07:55.415889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "score_ada_scaler = 0.4915254237288136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking key variables to predict the outcome.\n",
    "- Chi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:05:19.323546Z",
     "iopub.status.busy": "2025-01-31T11:05:19.322593Z",
     "iopub.status.idle": "2025-01-31T11:05:19.345598Z",
     "shell.execute_reply": "2025-01-31T11:05:19.344074Z",
     "shell.execute_reply.started": "2025-01-31T11:05:19.323456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "features = X\n",
    "target = y\n",
    "\n",
    "best_features = SelectKBest(score_func = chi2,k = 'all')\n",
    "fit = best_features.fit(features,target)\n",
    "\n",
    "featureScores = pd.DataFrame(data = fit.scores_,index = list(columns),columns = ['Chi Squared Score']) \n",
    "\n",
    "featureScores.sort_values(by = 'Chi Squared Score', ascending = False).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:05:51.404209Z",
     "iopub.status.busy": "2025-01-31T11:05:51.403784Z",
     "iopub.status.idle": "2025-01-31T11:05:51.412004Z",
     "shell.execute_reply": "2025-01-31T11:05:51.410844Z",
     "shell.execute_reply.started": "2025-01-31T11:05:51.404180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T11:06:08.397784Z",
     "iopub.status.busy": "2025-01-31T11:06:08.397345Z",
     "iopub.status.idle": "2025-01-31T11:06:08.406363Z",
     "shell.execute_reply": "2025-01-31T11:06:08.404826Z",
     "shell.execute_reply.started": "2025-01-31T11:06:08.397752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_imp_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T12:18:01.348803Z",
     "iopub.status.busy": "2025-01-31T12:18:01.348401Z",
     "iopub.status.idle": "2025-01-31T12:18:01.367706Z",
     "shell.execute_reply": "2025-01-31T12:18:01.366376Z",
     "shell.execute_reply.started": "2025-01-31T12:18:01.348777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Naive_dict_v1 = {'Model':'Naive Bayes',\n",
    "               'Scaling':'Normal Data',\n",
    "               'Type':'Gaussian',\n",
    "               'Precision':score_naive_gaussian}\n",
    "\n",
    "Decision_dict = {'Model':'Decision Tree',\n",
    "               'Scaling':'Normal Data',\n",
    "               'Type': 'Gini',\n",
    "               'Precision':score_tree}\n",
    "\n",
    "Random_dict = {'Model':'Random Forest',\n",
    "               'Scaling':'Normal Data',\n",
    "               'Type': 'Gini',\n",
    "               'Precision':score_random}\n",
    "\n",
    "\n",
    "KNN_dict_v2 = {'Model':'KNN',\n",
    "               'Scaling':'Normal',\n",
    "               'Type':'-',\n",
    "               'Precision':score_knn}\n",
    "\n",
    "Logistic_dict_v1 = {'Model':'Logistic Regression',\n",
    "               'Scaling':'Normal Data',\n",
    "               'Type':'-',\n",
    "               'Precision':logistic_normal}\n",
    "\n",
    "ada_dict_v1 = {'Model':'AdaBoost',\n",
    "               'Scaling':'StandardScaler',\n",
    "               'Type':'-',\n",
    "               'Precision':score_ada_scaler}\n",
    "\n",
    "resume = pd.DataFrame({'Naive Bayes':pd.Series(Naive_dict_v1),\n",
    "                       'Decision Tree':pd.Series(Decision_dict),\n",
    "                       'Random Forest':pd.Series(Random_dict),\n",
    "                       'KNN':pd.Series(KNN_dict_v2),\n",
    "                       'Logistic Regression':pd.Series(Logistic_dict_v1),\n",
    "                       'AdaBoost':pd.Series(ada_dict_v1)\n",
    "                      })\n",
    "\n",
    "resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 4. Conclusion </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To conclude the project, it's a very basic project because unfortunately we have very few data. The theme is interesting, the idea is quite intriguing, but unfortunately, we don’t have a good amount of data. We only have 6 columns with fairly interesting data, but what makes the work difficult is the fact that we only have 200 rows (our total population), which makes it quite challenging. Although we don’t have many columns, the data present makes it a promising project, but we lacked the quantity of data to carry out a deeper analysis.\n",
    "\n",
    "#### Looking at our dataset, we can see that we have some categorical variables and some continuous variables. We can also verify that we have 3 null values. Despite having a very limited amount of data, I chose to remove these 3 data points since they represent just 3 rows.\n",
    "\n",
    "#### Looking at our exploratory analysis, we can see that our dataset is quite well distributed. When we look at the gender variable, we see that there are more women than men in our dataset. We can also see that about 55% of our dataset uses a seatbelt/helmet (which I found alarming, considering that 45% don’t use them). When we look at our target variable, we can see that it is very well distributed, with nearly 50% of the data in each class (which is not a good value if the data is real).\n",
    "\n",
    "#### Looking at our continuous variables, we see a very similar pattern to the categorical variables in terms of data distribution. The data is very well distributed across our population, showing that we have all types of people available for analysis. There are some peaks, but still, the dataset is well distributed. When we look at the boxplots, we can see that we don’t have any outliers (which means there is no need to treat these data). Looking at the mean, we can verify that the average age of our population is 43, showing that we have an \"older\" population. The same applies when we look at the impact speed variable, with an average impact speed of 70 km/h.\n",
    "\n",
    "#### When we started our bivariate analysis, we were able to identify some interesting patterns despite not having much information about the accident. When we look at the gender variable, we observe a higher probability of a woman dying compared to a man, but as mentioned earlier, we cannot distinguish whether the woman was driving or just in the car, the type of accident, etc. Looking at the other two categorical variables, we are surprised to find that people who do not use helmets in this dataset are more likely to die than those who use them, which doesn’t make much sense. We see the opposite behavior when we analyze the use of seat belts (which makes a lot of sense to me, as using a seat belt makes it safer).\n",
    "\n",
    "#### When we look at our continuous variables compared to our target variable, we don’t find a very well-defined pattern, but we can see some differences. When we look at the age variable, we find that fatal accidents are more likely to happen to younger people (though not much difference). When we look at the other variable, which is impact speed, we don’t see a clear pattern, which is quite strange, as it automatically makes us think that the faster the person is driving, the more likely they are to die.\n",
    "\n",
    "#### Now, comparing the other variables to understand the behavior between them, when we compare the Age x Impact Speed variables to try to understand if there is a correlation between speed and age, we couldn’t find this differentiation, with a variety of different speeds regardless of age. When we look at age compared to the probability of using a seat belt/helmet, we didn’t find any defined pattern that differentiates age and the habit of using a seat belt/helmet. In our last graph, we can have another interesting insight where we see that most high-speed accidents are more likely to involve females. 25% of the data at the top for males range from 84 km/h to 120 km/h, while for females, it’s from 104 km/h to 120 km/h, showing a predominance of high speeds.\n",
    "\n",
    "#### For the Machine Learning part, we transformed our categorical variables into numeric variables using LabelEncoder, then scaled the data using StandardScaler. After that, we split our dataset into training and testing sets with a 70/30 percentage split. Speaking about our machine learning models, all of them performed poorly, with a terrible result in predicting both outcomes, averaging 52%. This suggests that we may be just a little better than flipping a coin to predict the outcome. This could be influenced by the small amount of data we have for training the machine learning model or perhaps even the data’s inability to explain the final result. I believe it’s a combination of both, with a predominance of the lack of data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
