{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580d31fe",
   "metadata": {
    "papermill": {
     "duration": 0.007875,
     "end_time": "2025-04-03T08:44:24.921054",
     "exception": false,
     "start_time": "2025-04-03T08:44:24.913179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IPL Dataset Analysis\n",
    "\n",
    "#### IPL data analysis using `matches.csv` and `deliveries.csv`. Includes preprocessing, cleaning, EDA (including batsman performance),feature engineering, and modeling preparation steps with Kaggle-compatible markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcbe1a",
   "metadata": {
    "papermill": {
     "duration": 0.006628,
     "end_time": "2025-04-03T08:44:24.934730",
     "exception": false,
     "start_time": "2025-04-03T08:44:24.928102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align:center; padding:15px; border-radius:15px; margin:20px 0;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/genieincodebottle/generative-ai/main/images/ipl-image.png\" \n",
    "         alt=\"IPL Team\" \n",
    "         style=\"width:100%; max-width:800px; border:5px solid #000000; border-radius:10px; display:block; margin:auto; box-shadow:0 4px 8px rgba(0,0,0,0.2);\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b26100",
   "metadata": {
    "papermill": {
     "duration": 0.006564,
     "end_time": "2025-04-03T08:44:24.948337",
     "exception": false,
     "start_time": "2025-04-03T08:44:24.941773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup: Install and Import Libraries\n",
    "\n",
    "#### First, make sure all necessary libraries are installed. Although most ML libraries are pre-installed on Kaggle, I noticed that the default Plotly library wasn’t functioning correctly and failed to render the required visualizations. To fix this, install Plotly using pip, then restart the kernel via Run → Restart & Clear All Outputs to apply the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ede0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:44:24.963329Z",
     "iopub.status.busy": "2025-04-03T08:44:24.962970Z",
     "iopub.status.idle": "2025-04-03T08:45:05.353049Z",
     "shell.execute_reply": "2025-04-03T08:45:05.351614Z"
    },
    "papermill": {
     "duration": 40.406251,
     "end_time": "2025-04-03T08:45:05.361311",
     "exception": false,
     "start_time": "2025-04-03T08:44:24.955060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -qU plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f3104",
   "metadata": {
    "papermill": {
     "duration": 0.007074,
     "end_time": "2025-04-03T08:45:05.375662",
     "exception": false,
     "start_time": "2025-04-03T08:45:05.368588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a999be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:05.391826Z",
     "iopub.status.busy": "2025-04-03T08:45:05.391406Z",
     "iopub.status.idle": "2025-04-03T08:45:09.298850Z",
     "shell.execute_reply": "2025-04-03T08:45:09.297929Z"
    },
    "papermill": {
     "duration": 3.917723,
     "end_time": "2025-04-03T08:45:09.300425",
     "exception": false,
     "start_time": "2025-04-03T08:45:05.382702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Plotly visualization\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "# --- SET KAGGLE RENDERER ---\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"kaggle\" \n",
    "print(f\"Plotly default renderer set to: {pio.renderers.default}\")\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Initialize plotly for notebook use if running in a notebook environment\n",
    "try:\n",
    "    init_notebook_mode(connected=True)\n",
    "except Exception as e:\n",
    "    print(f\"Plotly notebook mode initialization failed: {e}. Running in non-notebook mode.\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot aesthetics\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9011a09",
   "metadata": {
    "papermill": {
     "duration": 0.007088,
     "end_time": "2025-04-03T08:45:09.315136",
     "exception": false,
     "start_time": "2025-04-03T08:45:09.308048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Ingestion/Loading\n",
    "#### Load the `matches.csv` and `deliveries.csv` datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3764681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:09.331066Z",
     "iopub.status.busy": "2025-04-03T08:45:09.330472Z",
     "iopub.status.idle": "2025-04-03T08:45:10.189188Z",
     "shell.execute_reply": "2025-04-03T08:45:10.187607Z"
    },
    "papermill": {
     "duration": 0.868687,
     "end_time": "2025-04-03T08:45:10.190989",
     "exception": false,
     "start_time": "2025-04-03T08:45:09.322302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 1. DATA INGESTION/LOADING ----------\")\n",
    "\n",
    "try:\n",
    "    # Load the matches and deliveries datasets\n",
    "    matches_df = pd.read_csv('/Users/riteshkumar/Downloads/ML/IPL/matches.csv', \n",
    "                             na_values=['NA', ''])\n",
    "    deliveries_df = pd.read_csv('/Users/riteshkumar/Downloads/ML/IPL/deliveries.csv', \n",
    "                                na_values=['NA', ''])\n",
    "    \n",
    "    # Print loading success message and dataset dimensions\n",
    "    print(\"Successfully loaded matches.csv and deliveries.csv\")\n",
    "    print(f\"Matches dataset shape: {matches_df.shape}\")\n",
    "    print(f\"Deliveries dataset shape: {deliveries_df.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    print(\"Please ensure 'matches.csv' and 'deliveries.csv' are in the same directory.\")\n",
    "    exit()  # Exit if files are not found\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nMatches Data - First 5 rows:\")\n",
    "print(matches_df.head())\n",
    "\n",
    "print(\"\\nDeliveries Data - First 5 rows:\")\n",
    "print(deliveries_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16895e8",
   "metadata": {
    "papermill": {
     "duration": 0.007274,
     "end_time": "2025-04-03T08:45:10.206135",
     "exception": false,
     "start_time": "2025-04-03T08:45:10.198861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Data Exploration\n",
    "#### Examine the Structure, Types, Missing Values, and basic Statistics of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8484f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:10.223002Z",
     "iopub.status.busy": "2025-04-03T08:45:10.222651Z",
     "iopub.status.idle": "2025-04-03T08:45:10.816882Z",
     "shell.execute_reply": "2025-04-03T08:45:10.815577Z"
    },
    "papermill": {
     "duration": 0.604718,
     "end_time": "2025-04-03T08:45:10.818690",
     "exception": false,
     "start_time": "2025-04-03T08:45:10.213972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 2a. DATA EXPLORATION (Matches Data) ----------\")\n",
    "\n",
    "print(\"\\nMatches Data - Column information:\")\n",
    "matches_df.info()\n",
    "\n",
    "print(\"\\nMatches Data - Summary statistics for numerical columns:\")\n",
    "# Need to convert result_margin to numeric first if it's not already\n",
    "matches_df['result_margin'] = pd.to_numeric(matches_df['result_margin'], errors='coerce')\n",
    "print(matches_df.describe())\n",
    "\n",
    "print(\"\\nMatches Data - Missing values per column:\")\n",
    "print(matches_df.isnull().sum())\n",
    "\n",
    "print(f\"\\nMatches Data - Number of duplicate rows: {matches_df.duplicated().sum()}\")\n",
    "\n",
    "# --- 2b. DATA EXPLORATION (Deliveries Data) ---\n",
    "print(\"\\n---------- 2b. DATA EXPLORATION (Deliveries Data) ----------\")\n",
    "\n",
    "print(\"\\nDeliveries Data - Column information:\")\n",
    "deliveries_df.info()\n",
    "\n",
    "print(\"\\nDeliveries Data - Summary statistics for numerical columns:\")\n",
    "print(deliveries_df.describe())\n",
    "\n",
    "print(\"\\nDeliveries Data - Missing values per column:\")\n",
    "# Fill categorical NAs related to dismissal with 'No Dismissal' or similar for clarity\n",
    "deliveries_df['player_dismissed'] = deliveries_df['player_dismissed'].fillna('No Dismissal')\n",
    "deliveries_df['dismissal_kind'] = deliveries_df['dismissal_kind'].fillna('No Dismissal')\n",
    "deliveries_df['fielder'] = deliveries_df['fielder'].fillna('None')\n",
    "deliveries_df['extras_type'] = deliveries_df['extras_type'].fillna('None')  # Fill NA extras type\n",
    "\n",
    "# Re-check after filling\n",
    "print(deliveries_df.isnull().sum()) \n",
    "\n",
    "print(f\"\\nDeliveries Data - Number of duplicate rows: {deliveries_df.duplicated().sum()}\")\n",
    "# Note: Duplicates might be possible in deliveries data if re-bowls occur, context matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd9720",
   "metadata": {
    "papermill": {
     "duration": 0.009444,
     "end_time": "2025-04-03T08:45:10.836801",
     "exception": false,
     "start_time": "2025-04-03T08:45:10.827357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Data Cleaning\n",
    "#### Handle missing values, correct data types, standardize names, and manage potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7d23a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:10.854468Z",
     "iopub.status.busy": "2025-04-03T08:45:10.854123Z",
     "iopub.status.idle": "2025-04-03T08:45:11.035430Z",
     "shell.execute_reply": "2025-04-03T08:45:11.033967Z"
    },
    "papermill": {
     "duration": 0.192363,
     "end_time": "2025-04-03T08:45:11.037318",
     "exception": false,
     "start_time": "2025-04-03T08:45:10.844955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 3. DATA CLEANING (Matches Data) ----------\")\n",
    "\n",
    "# 3.1 Handle missing values\n",
    "print(\"\\n3.1 Handling missing values (Matches)\")\n",
    "# First, make a copy to preserve the original data\n",
    "matches_clean = matches_df.copy()\n",
    "\n",
    "# Handle missing winner, result, result_margin, player_of_match based on context\n",
    "# If 'result' is 'tie' or 'no result', winner might be NaN legitimately.\n",
    "# Fill 'winner' and 'player_of_match' for 'no result' matches\n",
    "no_result_indices = matches_clean[matches_clean['result'] == 'no result'].index\n",
    "matches_clean.loc[no_result_indices, 'winner'] = matches_clean.loc[no_result_indices, 'winner'].fillna('No Result')\n",
    "matches_clean.loc[no_result_indices, 'player_of_match'] = matches_clean.loc[no_result_indices, 'player_of_match'].fillna('Not Awarded')\n",
    "\n",
    "# Fill remaining NaNs in winner/player_of_match where a result exists (might indicate data entry issue)\n",
    "# For simplicity, filling with 'Unknown' or 'Not Awarded'\n",
    "matches_clean['winner'] = matches_clean['winner'].fillna('Unknown')\n",
    "matches_clean['player_of_match'] = matches_clean['player_of_match'].fillna('Not Awarded')\n",
    "\n",
    "# Fill missing city with 'Unknown'\n",
    "matches_clean['city'] = matches_clean['city'].fillna('Unknown')\n",
    "\n",
    "# Fill missing result_margin with 0 (assuming 0 margin if NaN)\n",
    "matches_clean['result_margin'] = matches_clean['result_margin'].fillna(0)\n",
    "\n",
    "# Handle method (like DL) - Fill NaN method with 'Normal'\n",
    "matches_clean['method'] = matches_clean['method'].fillna('Normal')\n",
    "\n",
    "# Fill missing umpires with 'Unknown'\n",
    "matches_clean['umpire1'] = matches_clean['umpire1'].fillna('Unknown')\n",
    "matches_clean['umpire2'] = matches_clean['umpire2'].fillna('Unknown')\n",
    "\n",
    "# Derive win_by_runs and win_by_wickets from result and result_margin\n",
    "matches_clean['win_by_runs'] = np.where(\n",
    "    matches_clean['result'] == 'runs', \n",
    "    matches_clean['result_margin'], \n",
    "    0\n",
    ").astype(int)\n",
    "\n",
    "matches_clean['win_by_wickets'] = np.where(\n",
    "    matches_clean['result'] == 'wickets', \n",
    "    matches_clean['result_margin'], \n",
    "    0\n",
    ").astype(int)\n",
    "\n",
    "# Check remaining missing values\n",
    "print(\"\\nRemaining missing values after cleaning (Matches):\")\n",
    "print(matches_clean.isnull().sum())\n",
    "\n",
    "\n",
    "# 3.2 Fix data types (Matches)\n",
    "print(\"\\n3.2 Fixing data types (Matches)\")\n",
    "# Convert date to datetime\n",
    "matches_clean['date'] = pd.to_datetime(matches_clean['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Extract date components\n",
    "matches_clean['year'] = matches_clean['date'].dt.year\n",
    "matches_clean['month'] = matches_clean['date'].dt.month\n",
    "matches_clean['day'] = matches_clean['date'].dt.day\n",
    "matches_clean['day_of_week'] = matches_clean['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "matches_clean['is_weekend'] = matches_clean['day_of_week'].isin([5, 6]).astype(int)  # 5=Saturday, 6=Sunday\n",
    "\n",
    "# Convert season to starting year\n",
    "matches_clean['season_year'] = matches_clean['season'].astype(str).str.split('/').str[0].astype(int)\n",
    "\n",
    "# Convert boolean-like columns\n",
    "matches_clean['super_over'] = matches_clean['super_over'].map({'Y': 1, 'N': 0}).fillna(0).astype(int)\n",
    "# Add a DL applied column based on method\n",
    "matches_clean['dl_applied'] = np.where(matches_clean['method'] == 'D/L', 1, 0).astype(int)\n",
    "\n",
    "print(\"\\nMatches Data types after conversion:\")\n",
    "print(matches_clean.dtypes[['date', 'year', 'month', 'day', 'day_of_week', \n",
    "                            'is_weekend', 'season_year', 'super_over', \n",
    "                            'dl_applied', 'win_by_runs', 'win_by_wickets']])\n",
    "\n",
    "\n",
    "# 3.3 Check and fix inconsistencies (Matches)\n",
    "print(\"\\n3.3 Checking and fixing inconsistencies (Matches)\")\n",
    "print(\"\\nUnique team names in team1:\")\n",
    "print(sorted(matches_clean['team1'].unique()))\n",
    "\n",
    "# Example of fixing inconsistencies (apply based on unique names list)\n",
    "team_name_mapping = {\n",
    "    'Rising Pune Supergiants': 'Rising Pune Supergiant',  # Example\n",
    "    # Add more mappings if needed after inspecting unique names\n",
    "}\n",
    "\n",
    "# Apply team name corrections to all relevant columns\n",
    "matches_clean['team1'] = matches_clean['team1'].replace(team_name_mapping)\n",
    "matches_clean['team2'] = matches_clean['team2'].replace(team_name_mapping)\n",
    "matches_clean['toss_winner'] = matches_clean['toss_winner'].replace(team_name_mapping)\n",
    "matches_clean['winner'] = matches_clean['winner'].replace(team_name_mapping)\n",
    "\n",
    "# Update batting/bowling teams in deliveries_df too\n",
    "deliveries_df['batting_team'] = deliveries_df['batting_team'].replace(team_name_mapping)\n",
    "deliveries_df['bowling_team'] = deliveries_df['bowling_team'].replace(team_name_mapping)\n",
    "\n",
    "\n",
    "# 3.4 Handling outliers (Matches)\n",
    "print(\"\\n3.4 Handling outliers (Matches)\")\n",
    "\n",
    "# Create boxplots for derived win_by_runs and win_by_wickets\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(\n",
    "    y=matches_clean[matches_clean['win_by_runs'] > 0]['win_by_runs'],  # Filter 0 values\n",
    "    name='Wins by Runs', \n",
    "    boxmean=True, \n",
    "    marker_color='blue'\n",
    "))\n",
    "fig.add_trace(go.Box(\n",
    "    y=matches_clean[matches_clean['win_by_wickets'] > 0]['win_by_wickets'],  # Filter 0 values\n",
    "    name='Wins by Wickets', \n",
    "    boxmean=True, \n",
    "    marker_color='green'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Distribution of Win Margins (Excluding Zero)',\n",
    "    yaxis_title='Margin Value', \n",
    "    boxmode='group'\n",
    ")\n",
    "fig.show()\n",
    "print(\"No outlier removal needed as extreme values are valid cricket match outcomes.\")\n",
    "\n",
    "\n",
    "# 3.5 Data Cleaning (Deliveries)\n",
    "print(\"\\n3.5 Data Cleaning (Deliveries)\")\n",
    "# Convert relevant columns to numeric, coercing errors\n",
    "num_cols_del = ['batsman_runs', 'extra_runs', 'total_runs', 'is_wicket']\n",
    "for col in num_cols_del:\n",
    "    deliveries_df[col] = pd.to_numeric(deliveries_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# Ensure IDs are consistent types if merging later\n",
    "matches_clean['id'] = matches_clean['id'].astype(int)\n",
    "deliveries_df['match_id'] = deliveries_df['match_id'].astype(int)\n",
    "\n",
    "print(\"Deliveries data types checked/converted.\")\n",
    "print(deliveries_df[num_cols_del + ['match_id']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83578efe",
   "metadata": {
    "papermill": {
     "duration": 0.011887,
     "end_time": "2025-04-03T08:45:11.062221",
     "exception": false,
     "start_time": "2025-04-03T08:45:11.050334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Exploratory Data Analysis (EDA)\n",
    "#### Visualize patterns and trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22b8d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:11.089082Z",
     "iopub.status.busy": "2025-04-03T08:45:11.088717Z",
     "iopub.status.idle": "2025-04-03T08:45:12.619783Z",
     "shell.execute_reply": "2025-04-03T08:45:12.618598Z"
    },
    "papermill": {
     "duration": 1.548155,
     "end_time": "2025-04-03T08:45:12.623166",
     "exception": false,
     "start_time": "2025-04-03T08:45:11.075011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 4. EXPLORATORY DATA ANALYSIS (Matches) ----------\")\n",
    "\n",
    "# 4.1 Distribution of matches across seasons\n",
    "print(\"\\n4.1 Distribution of matches across seasons\")\n",
    "season_counts = matches_clean['season_year'].value_counts().sort_index()\n",
    "print(season_counts)\n",
    "fig1 = px.bar(x=season_counts.index, y=season_counts.values,\n",
    "              title='Number of IPL Matches per Season',\n",
    "              labels={'x': 'Season Year', 'y': 'Number of Matches'})\n",
    "fig1.update_layout(xaxis_tickangle=-45)\n",
    "fig1.show()\n",
    "\n",
    "# 4.2 Most successful teams\n",
    "print(\"\\n4.2 Most successful teams\")\n",
    "# Exclude 'No Result' and 'Unknown' winners\n",
    "team_wins = matches_clean[~matches_clean['winner'].isin(['No Result', 'Unknown'])]['winner'].value_counts()\n",
    "print(team_wins.head(10))\n",
    "top_teams = team_wins.head(10).sort_values(ascending=True)\n",
    "fig2 = px.bar(x=top_teams.values, y=top_teams.index, orientation='h',\n",
    "              title='Top 10 Teams with Most Wins in IPL',\n",
    "              labels={'x': 'Number of Wins', 'y': 'Team'})\n",
    "fig2.show()\n",
    "\n",
    "# 4.3 Toss decision analysis\n",
    "print(\"\\n4.3 Toss decision analysis\")\n",
    "toss_decisions = matches_clean['toss_decision'].value_counts()\n",
    "print(toss_decisions)\n",
    "fig3 = px.pie(values=toss_decisions.values, names=toss_decisions.index,\n",
    "              title='Toss Decisions in IPL')\n",
    "fig3.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig3.show()\n",
    "\n",
    "# 4.4 Impact of toss on match result\n",
    "print(\"\\n4.4 Impact of toss on match result\")\n",
    "matches_clean['toss_winner_won_match'] = (matches_clean['toss_winner'] == matches_clean['winner']).astype(int)\n",
    "# Exclude matches with no result or unknown winner for calculating meaningful percentage\n",
    "valid_matches_for_toss_impact = matches_clean[~matches_clean['winner'].isin(['No Result', 'Unknown'])]\n",
    "toss_win_percentage = valid_matches_for_toss_impact['toss_winner_won_match'].mean() * 100\n",
    "print(f\"Percentage of matches (with result) where toss winner also won: {toss_win_percentage:.2f}%\")\n",
    "\n",
    "toss_decision_impact = valid_matches_for_toss_impact.groupby('toss_decision')['toss_winner_won_match'].mean() * 100\n",
    "print(\"\\nWin percentage when winning toss & choosing to bat vs field:\")\n",
    "print(toss_decision_impact)\n",
    "fig4 = px.bar(x=toss_decision_impact.index, y=toss_decision_impact.values,\n",
    "              title='Match Win % When Winning Toss (by Decision)',\n",
    "              labels={'x': 'Toss Decision', 'y': 'Win Percentage'})\n",
    "fig4.update_layout(yaxis_range=[0, 100])\n",
    "fig4.show()\n",
    "\n",
    "# 4.5 Venue analysis\n",
    "print(\"\\n4.5 Venue analysis\")\n",
    "venue_counts = matches_clean['venue'].value_counts().head(10)\n",
    "print(\"Top 10 venues by number of matches:\")\n",
    "print(venue_counts)\n",
    "fig5 = px.bar(x=venue_counts.values, y=venue_counts.index, orientation='h',\n",
    "              title='Top 10 IPL Venues by Number of Matches',\n",
    "              labels={'x': 'Number of Matches', 'y': 'Venue'})\n",
    "fig5.update_xaxes(title_text=\"Number of Matches\")\n",
    "fig5.update_yaxes(title_text=\"Venue\")\n",
    "fig5.show()\n",
    "\n",
    "\n",
    "# 4.6 Player of the match analysis\n",
    "print(\"\\n4.6 Player of the match analysis\")\n",
    "player_of_match_counts = matches_clean[matches_clean['player_of_match'] != 'Not Awarded']['player_of_match'].value_counts().head(10)\n",
    "print(\"Top 10 players with most Player of the Match awards:\")\n",
    "print(player_of_match_counts)\n",
    "fig6 = px.bar(x=player_of_match_counts.values, y=player_of_match_counts.index, orientation='h',\n",
    "              title='Top 10 Players with Most Player of the Match Awards',\n",
    "              labels={'x': 'Number of Awards', 'y': 'Player'})\n",
    "fig6.update_xaxes(title_text=\"Number of Awards\")\n",
    "fig6.update_yaxes(title_text=\"Player\")\n",
    "fig6.show()\n",
    "\n",
    "# 4.7 Win margin analysis\n",
    "print(\"\\n4.7 Win margin analysis\")\n",
    "# Distribution of win by runs (excluding 0)\n",
    "fig7 = px.histogram(matches_clean[matches_clean['win_by_runs'] > 0], x='win_by_runs', nbins=20,\n",
    "                    title='Distribution of Victory Margin (Runs)')\n",
    "fig7.update_layout(xaxis_title='Runs', yaxis_title='Frequency')\n",
    "fig7.show()\n",
    "\n",
    "# Distribution of win by wickets (excluding 0)\n",
    "fig8 = px.histogram(matches_clean[matches_clean['win_by_wickets'] > 0], x='win_by_wickets', nbins=10,\n",
    "                    title='Distribution of Victory Margin (Wickets)')\n",
    "fig8.update_layout(xaxis_title='Wickets', yaxis_title='Frequency')\n",
    "fig8.show()\n",
    "\n",
    "# 4.8 Match outcomes over time (trend analysis)\n",
    "print(\"\\n4.8 Match outcomes over time\")\n",
    "runs_by_season = matches_clean.groupby('season_year')['win_by_runs'].mean()\n",
    "wickets_by_season = matches_clean.groupby('season_year')['win_by_wickets'].mean()\n",
    "\n",
    "fig9 = px.line(x=runs_by_season.index, y=runs_by_season.values, markers=True,\n",
    "               title='Average Win Margin (Runs) by Season')\n",
    "fig9.update_layout(xaxis_title='Season Year', yaxis_title='Average Runs')\n",
    "fig9.show()\n",
    "\n",
    "fig10 = px.line(x=wickets_by_season.index, y=wickets_by_season.values, markers=True,\n",
    "                title='Average Win Margin (Wickets) by Season')\n",
    "fig10.update_layout(xaxis_title='Season Year', yaxis_title='Average Wickets')\n",
    "fig10.show()\n",
    "\n",
    "\n",
    "# --- 4b. EXPLORATORY DATA ANALYSIS (EDA) - Deliveries (Batsman Performance) ---\n",
    "print(\"\\n---------- 4b. EXPLORATORY DATA ANALYSIS (Deliveries - Batsman) ----------\")\n",
    "\n",
    "# Merge deliveries with matches to get season context\n",
    "deliveries_merged = pd.merge(deliveries_df, matches_clean[['id', 'season_year']], left_on='match_id', right_on='id', how='left')\n",
    "\n",
    "# 4b.1 Top Run Scorers (Overall)\n",
    "print(\"\\n4b.1 Top Run Scorers (Overall)\")\n",
    "batsman_runs = deliveries_merged.groupby('batter')['batsman_runs'].sum().sort_values(ascending=False)\n",
    "print(batsman_runs.head(10))\n",
    "\n",
    "# Visualize Top 10 Run Scorers\n",
    "top_10_scorers = batsman_runs.head(10)\n",
    "fig11 = px.bar(x=top_10_scorers.index, y=top_10_scorers.values,\n",
    "               title='Top 10 Run Scorers in IPL History',\n",
    "               labels={'x': 'Batsman', 'y': 'Total Runs'})\n",
    "fig11.show()\n",
    "\n",
    "# 4b.2 Batsman Strike Rates (Overall, min 500 balls faced)\n",
    "print(\"\\n4b.2 Batsman Strike Rates (Overall, min 500 balls faced)\")\n",
    "# Exclude wides for balls faced count, but include no-balls, legbyes etc. as faced balls\n",
    "balls_faced = deliveries_merged[deliveries_merged['extras_type'] != 'wides'].groupby('batter')['ball'].count()\n",
    "batsman_strike_rate = (batsman_runs / balls_faced * 100)\n",
    "\n",
    "# Filter for batsmen with minimum balls faced\n",
    "min_balls_threshold = 500\n",
    "batsman_sr_filtered = batsman_strike_rate[balls_faced >= min_balls_threshold].sort_values(ascending=False)\n",
    "print(batsman_sr_filtered.head(10))\n",
    "\n",
    "# Visualize Top 10 Strike Rates\n",
    "top_10_sr = batsman_sr_filtered.head(10)\n",
    "fig12 = px.bar(x=top_10_sr.index, y=top_10_sr.values,\n",
    "               title=f'Top 10 Batsman Strike Rates (Min {min_balls_threshold} Balls Faced)',\n",
    "               labels={'x': 'Batsman', 'y': 'Strike Rate'})\n",
    "fig12.show()\n",
    "\n",
    "# 4b.3 Batsman Averages (Overall, min 20 dismissals)\n",
    "print(\"\\n4b.3 Batsman Averages (Overall, min 20 dismissals)\")\n",
    "dismissals = deliveries_merged[deliveries_merged['is_wicket'] == 1]['player_dismissed'].value_counts()\n",
    "# Ensure dismissals index matches batsman_runs index (handle potential name mismatches if any)\n",
    "dismissals = dismissals.reindex(batsman_runs.index, fill_value=0)\n",
    "\n",
    "batsman_average = batsman_runs / dismissals\n",
    "# Handle cases where batsman was never dismissed (average is infinity) -> replace with total runs or NaN\n",
    "batsman_average.replace([np.inf, -np.inf], np.nan, inplace=True) # Or fillna(batsman_runs)\n",
    "\n",
    "min_dismissals_threshold = 20\n",
    "batsman_avg_filtered = batsman_average[dismissals >= min_dismissals_threshold].sort_values(ascending=False)\n",
    "print(batsman_avg_filtered.head(10))\n",
    "\n",
    "# Visualize Top 10 Averages\n",
    "top_10_avg = batsman_avg_filtered.head(10)\n",
    "fig13 = px.bar(x=top_10_avg.index, y=top_10_avg.values,\n",
    "               title=f'Top 10 Batsman Averages (Min {min_dismissals_threshold} Dismissals)',\n",
    "               labels={'x': 'Batsman', 'y': 'Average Runs'})\n",
    "fig13.show()\n",
    "\n",
    "# 4b.4 Most Sixes (Overall)\n",
    "print(\"\\n4b.4 Most Sixes (Overall)\")\n",
    "sixes = deliveries_merged[deliveries_merged['batsman_runs'] == 6].groupby('batter')['batsman_runs'].count().sort_values(ascending=False)\n",
    "print(sixes.head(10))\n",
    "\n",
    "# Visualize Top 10 Six Hitters\n",
    "top_10_sixes = sixes.head(10)\n",
    "fig14 = px.bar(x=top_10_sixes.index, y=top_10_sixes.values,\n",
    "               title='Top 10 Batsmen by Number of Sixes Hit',\n",
    "               labels={'x': 'Batsman', 'y': 'Number of Sixes'})\n",
    "fig14.show()\n",
    "\n",
    "# 4b.5 Top Run Scorers per Season\n",
    "print(\"\\n4b.5 Top Run Scorers per Season\")\n",
    "runs_per_season = deliveries_merged.groupby(['season_year', 'batter'])['batsman_runs'].sum().reset_index()\n",
    "# Find the top scorer for each season\n",
    "top_scorer_per_season = runs_per_season.loc[runs_per_season.groupby('season_year')['batsman_runs'].idxmax()]\n",
    "print(top_scorer_per_season.sort_values('season_year'))\n",
    "\n",
    "fig15 = px.bar(top_scorer_per_season.sort_values('season_year'), x='season_year', y='batsman_runs', color='batter',\n",
    "               title='Top Run Scorer (Orange Cap) Each Season',\n",
    "               labels={'season_year': 'Season', 'batsman_runs': 'Total Runs', 'batter': 'Batsman'})\n",
    "fig15.update_layout(xaxis={'type': 'category'}) # Treat season year as category\n",
    "fig15.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658420e0",
   "metadata": {
    "papermill": {
     "duration": 0.075827,
     "end_time": "2025-04-03T08:45:12.779647",
     "exception": false,
     "start_time": "2025-04-03T08:45:12.703820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Feature Engineering\n",
    "#### Create new features from existing data to potentially improve model performance or gain deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df6ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:12.936939Z",
     "iopub.status.busy": "2025-04-03T08:45:12.936561Z",
     "iopub.status.idle": "2025-04-03T08:45:22.531027Z",
     "shell.execute_reply": "2025-04-03T08:45:22.529615Z"
    },
    "papermill": {
     "duration": 9.674532,
     "end_time": "2025-04-03T08:45:22.532953",
     "exception": false,
     "start_time": "2025-04-03T08:45:12.858421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 5. FEATURE ENGINEERING (Matches Data) ----------\")\n",
    "\n",
    "# 5.1 Team matchups\n",
    "print(\"\\n5.1 Team matchups\")\n",
    "matches_clean['matchup'] = matches_clean.apply(\n",
    "    lambda row: f\"{sorted([row['team1'], row['team2']])[0]} vs {sorted([row['team1'], row['team2']])[1]}\", axis=1\n",
    ")\n",
    "matchup_counts = matches_clean['matchup'].value_counts().head(10)\n",
    "print(\"Top 10 most frequent team matchups:\")\n",
    "print(matchup_counts)\n",
    "\n",
    "# 5.2 Team performance metrics (Win Rate)\n",
    "print(\"\\n5.2 Team performance metrics (Win Rate)\")\n",
    "# Consider only matches with a definitive winner\n",
    "valid_matches = matches_clean[~matches_clean['winner'].isin(['No Result', 'Unknown'])]\n",
    "team_matches_played = pd.concat([valid_matches['team1'], valid_matches['team2']]).value_counts()\n",
    "team_wins = valid_matches['winner'].value_counts()\n",
    "team_win_rate = (team_wins / team_matches_played).fillna(0).sort_values(ascending=False)\n",
    "print(\"Win rate for each team (in matches with a result):\")\n",
    "print(team_win_rate)\n",
    "\n",
    "# Visualize Win Rates\n",
    "fig_win_rate = px.bar(x=team_win_rate.index, y=team_win_rate.values,\n",
    "                      title='Overall Team Win Rates (in Matches with Result)',\n",
    "                      labels={'x': 'Team', 'y': 'Win Rate'})\n",
    "fig_win_rate.update_layout(yaxis_tickformat=\".1%\")\n",
    "fig_win_rate.show()\n",
    "\n",
    "# 5.3 Venue-based features (Win rate by venue for each team)\n",
    "print(\"\\n5.3 Venue-based features (Win rate by venue)\")\n",
    "team_venue_combinations = []\n",
    "# Ensure we iterate over teams present in the cleaned data\n",
    "teams_in_data = pd.concat([matches_clean['team1'], matches_clean['team2']]).unique()\n",
    "venues_in_data = matches_clean['venue'].unique()\n",
    "\n",
    "for team in teams_in_data:\n",
    "    if team is None or pd.isna(team): continue # Skip potential None values\n",
    "    for venue in venues_in_data:\n",
    "        if venue is None or pd.isna(venue): continue # Skip potential None values\n",
    "\n",
    "        # Find matches where the team played at the venue\n",
    "        team_matches_at_venue = matches_clean[((matches_clean['team1'] == team) | (matches_clean['team2'] == team)) & (matches_clean['venue'] == venue)]\n",
    "        matches_count = len(team_matches_at_venue)\n",
    "\n",
    "        if matches_count > 0:\n",
    "            # Find wins for the team at that venue\n",
    "            team_wins_at_venue = team_matches_at_venue[team_matches_at_venue['winner'] == team]\n",
    "            wins_count = len(team_wins_at_venue)\n",
    "            win_rate = wins_count / matches_count\n",
    "            team_venue_combinations.append({\n",
    "                'team': team, 'venue': venue, 'matches': matches_count,\n",
    "                'wins': wins_count, 'win_rate': win_rate\n",
    "            })\n",
    "\n",
    "venue_performance = pd.DataFrame(team_venue_combinations)\n",
    "venue_performance = venue_performance.sort_values(['win_rate', 'matches'], ascending=[False, False])\n",
    "\n",
    "print(\"Top 10 team-venue combinations by win rate (min 5 matches):\")\n",
    "print(venue_performance[venue_performance['matches'] >= 5].head(10))\n",
    "\n",
    "\n",
    "# 5.4 Home advantage (Simplified)\n",
    "print(\"\\n5.4 Home advantage\")\n",
    "# Using approximate city-team mapping (can be refined)\n",
    "team_home_city = {\n",
    "    'Mumbai Indians': 'Mumbai', 'Chennai Super Kings': 'Chennai',\n",
    "    'Kolkata Knight Riders': 'Kolkata', 'Royal Challengers Bangalore': 'Bangalore',\n",
    "    'Delhi Daredevils': 'Delhi', 'Delhi Capitals': 'Delhi', # Added Delhi Capitals\n",
    "    'Kings XI Punjab': 'Chandigarh', # Mohali is often listed as Chandigarh\n",
    "    'Punjab Kings': 'Chandigarh', # Added Punjab Kings\n",
    "    'Rajasthan Royals': 'Jaipur', 'Sunrisers Hyderabad': 'Hyderabad',\n",
    "    'Deccan Chargers': 'Hyderabad', # Added Deccan Chargers\n",
    "    'Pune Warriors': 'Pune', 'Rising Pune Supergiant': 'Pune',\n",
    "    'Gujarat Lions': 'Rajkot' # Main home ground\n",
    "}\n",
    "\n",
    "matches_clean['is_team1_home'] = matches_clean.apply(\n",
    "    lambda row: 1 if team_home_city.get(row['team1']) == row['city'] else 0, axis=1)\n",
    "matches_clean['is_team2_home'] = matches_clean.apply(\n",
    "    lambda row: 1 if team_home_city.get(row['team2']) == row['city'] else 0, axis=1)\n",
    "\n",
    "# Analyze impact of home advantage\n",
    "home_advantage_stats = []\n",
    "for team in team_matches_played.index:\n",
    "    # Team playing as Team 1 at home\n",
    "    t1_home_matches = matches_clean[(matches_clean['team1'] == team) & (matches_clean['is_team1_home'] == 1)]\n",
    "    t1_home_wins = t1_home_matches[t1_home_matches['winner'] == team]\n",
    "\n",
    "    # Team playing as Team 2 at home\n",
    "    t2_home_matches = matches_clean[(matches_clean['team2'] == team) & (matches_clean['is_team2_home'] == 1)]\n",
    "    t2_home_wins = t2_home_matches[t2_home_matches['winner'] == team]\n",
    "\n",
    "    total_home_matches = len(t1_home_matches) + len(t2_home_matches)\n",
    "    total_home_wins = len(t1_home_wins) + len(t2_home_wins)\n",
    "\n",
    "    if total_home_matches > 0:\n",
    "        home_win_rate = total_home_wins / total_home_matches\n",
    "        home_advantage_stats.append({'team': team, 'home_matches': total_home_matches, 'home_win_rate': home_win_rate})\n",
    "\n",
    "home_adv_df = pd.DataFrame(home_advantage_stats).sort_values('home_win_rate', ascending=False)\n",
    "print(\"\\nHome Win Rate by Team (Simplified):\")\n",
    "print(home_adv_df[home_adv_df['home_matches'] >= 5]) # Min 5 home matches\n",
    "\n",
    "\n",
    "# 5.5 Momentum/Form Features (Using Matches Data - Simplified)\n",
    "print(\"\\n5.5 Momentum and form features (Matches - Simplified)\")\n",
    "# Sort data by date\n",
    "matches_clean = matches_clean.sort_values('date')\n",
    "\n",
    "def get_recent_form(team, match_date, df, n_matches=5):\n",
    "    \"\"\"Calculate team's win rate in last n matches before given date\"\"\"\n",
    "    previous_matches = df[\n",
    "        ((df['team1'] == team) | (df['team2'] == team)) &\n",
    "        (df['date'] < match_date)\n",
    "    ].sort_values('date', ascending=False).head(n_matches)\n",
    "\n",
    "    if len(previous_matches) == 0:\n",
    "        return 0.5 # Default form if no previous matches\n",
    "\n",
    "    wins = sum(previous_matches['winner'] == team)\n",
    "    # Exclude no results from calculation base if needed\n",
    "    valid_prev_matches = previous_matches[~previous_matches['winner'].isin(['No Result', 'Unknown'])]\n",
    "    if len(valid_prev_matches) == 0:\n",
    "        return 0.5 # Default if no valid previous results\n",
    "    return wins / len(valid_prev_matches)\n",
    "\n",
    "# Apply form calculation (can be slow on full dataset)\n",
    "matches_clean['team1_form'] = matches_clean.apply(lambda row: get_recent_form(row['team1'], row['date'], matches_clean), axis=1)\n",
    "matches_clean['team2_form'] = matches_clean.apply(lambda row: get_recent_form(row['team2'], row['date'], matches_clean), axis=1)\n",
    "\n",
    "\n",
    "# 5.6 Seasonal Phase Features\n",
    "print(\"\\n5.6 Seasonal features\")\n",
    "season_dates = matches_clean.groupby('season_year')['date'].agg(['min', 'max'])\n",
    "\n",
    "def get_match_phase(row, season_dates_lookup):\n",
    "    if row['season_year'] not in season_dates_lookup.index:\n",
    "        return 'Unknown' # Handle cases where season might be missing in lookup\n",
    "    season_start = season_dates_lookup.loc[row['season_year'], 'min']\n",
    "    season_end = season_dates_lookup.loc[row['season_year'], 'max']\n",
    "    season_duration = (season_end - season_start).days\n",
    "\n",
    "    if season_duration <= 0: return 'Start' # Handle single-day seasons or issues\n",
    "\n",
    "    days_from_start = (row['date'] - season_start).days\n",
    "    position_in_season = days_from_start / season_duration\n",
    "\n",
    "    if position_in_season < 0.33: return 'Start'\n",
    "    elif position_in_season < 0.67: return 'Middle'\n",
    "    else: return 'End'\n",
    "\n",
    "matches_clean['season_phase'] = matches_clean.apply(lambda row: get_match_phase(row, season_dates), axis=1)\n",
    "\n",
    "# Analyze toss win impact by season phase\n",
    "season_phase_stats = valid_matches_for_toss_impact.groupby(matches_clean['season_phase'])['toss_winner_won_match'].mean() * 100\n",
    "print(\"Impact of winning toss on match outcome by season phase:\")\n",
    "print(season_phase_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62f93e",
   "metadata": {
    "papermill": {
     "duration": 0.09193,
     "end_time": "2025-04-03T08:45:22.781748",
     "exception": false,
     "start_time": "2025-04-03T08:45:22.689818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Preparing Data for Modeling\n",
    "#### Select features, encode categorical variables, scale numerical data, and split into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9107dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:22.949883Z",
     "iopub.status.busy": "2025-04-03T08:45:22.949479Z",
     "iopub.status.idle": "2025-04-03T08:45:23.330958Z",
     "shell.execute_reply": "2025-04-03T08:45:23.329346Z"
    },
    "papermill": {
     "duration": 0.468075,
     "end_time": "2025-04-03T08:45:23.333522",
     "exception": false,
     "start_time": "2025-04-03T08:45:22.865447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 6. PREPARING DATA FOR MODELING ----------\")\n",
    "\n",
    "# 6.1 One-hot encoding for categorical variables\n",
    "print(\"\\n6.1 One-hot encoding\")\n",
    "# Select features for modeling (example subset)\n",
    "features_for_model = ['season_year', 'win_by_runs', 'win_by_wickets', 'dl_applied', 'super_over',\n",
    "                      'is_weekend', 'toss_winner_won_match', 'is_team1_home', 'is_team2_home',\n",
    "                      'team1', 'team2', 'toss_winner', 'toss_decision', 'city', 'venue', 'season_phase']\n",
    "model_df = matches_clean[features_for_model].copy()\n",
    "\n",
    "# Handle potential remaining NaNs before encoding (should be minimal after cleaning)\n",
    "model_df.dropna(subset=['team1', 'team2', 'toss_winner', 'venue', 'city', 'toss_decision'], inplace=True) # Drop rows where key categoricals are missing\n",
    "\n",
    "cat_columns = ['team1', 'team2', 'toss_winner', 'toss_decision', 'city', 'venue', 'season_phase']\n",
    "model_df_encoded = pd.get_dummies(model_df, columns=cat_columns, drop_first=True, dummy_na=False) # dummy_na=False explicit\n",
    "\n",
    "print(f\"Shape after one-hot encoding: {model_df_encoded.shape}\")\n",
    "print(f\"Number of features: {model_df_encoded.shape[1]}\")\n",
    "\n",
    "# 6.2 Feature scaling\n",
    "print(\"\\n6.2 Feature scaling\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Identify numerical columns to scale (excluding binary/encoded ones)\n",
    "numerical_cols = ['season_year', 'win_by_runs', 'win_by_wickets'] # Add other relevant numerical features if used\n",
    "cols_to_scale = [col for col in numerical_cols if col in model_df_encoded.columns]\n",
    "\n",
    "if cols_to_scale:\n",
    "    model_df_encoded[cols_to_scale] = scaler.fit_transform(model_df_encoded[cols_to_scale])\n",
    "    print(\"Sample of scaled features:\")\n",
    "    print(model_df_encoded[cols_to_scale].head())\n",
    "else:\n",
    "    print(\"No numerical columns found to scale.\")\n",
    "\n",
    "\n",
    "# 6.3 Train-test split\n",
    "print(\"\\n6.3 Train-test split\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: Predicting if team1 wins\n",
    "# Define target variable (ensure alignment after potential row drops)\n",
    "# Target: 1 if team1 wins, 0 otherwise (team2 wins or no result/tie handled by winner column)\n",
    "# Re-align target 'y' with the potentially filtered model_df_encoded index\n",
    "target_series = (matches_clean.loc[model_df_encoded.index, 'winner'] == matches_clean.loc[model_df_encoded.index, 'team1']).astype(int)\n",
    "\n",
    "X = model_df_encoded.drop(columns=['toss_winner_won_match']) # Assuming toss_winner_won_match is not used as direct feature\n",
    "y = target_series\n",
    "\n",
    "if len(X) == len(y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y) # Stratify for classification\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "else:\n",
    "    print(f\"Error: Mismatch between feature count ({len(X)}) and target count ({len(y)}). Skipping train-test split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c667969",
   "metadata": {
    "papermill": {
     "duration": 0.170869,
     "end_time": "2025-04-03T08:45:23.665999",
     "exception": false,
     "start_time": "2025-04-03T08:45:23.495130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Advanced Feature Engineering (Optional)\n",
    "#### Explore more complex features like Elo ratings or interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2eb8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:24.012195Z",
     "iopub.status.busy": "2025-04-03T08:45:24.011781Z",
     "iopub.status.idle": "2025-04-03T08:45:24.138979Z",
     "shell.execute_reply": "2025-04-03T08:45:24.137614Z"
    },
    "papermill": {
     "duration": 0.289287,
     "end_time": "2025-04-03T08:45:24.140889",
     "exception": false,
     "start_time": "2025-04-03T08:45:23.851602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 7. ADVANCED FEATURE ENGINEERING ----------\")\n",
    "\n",
    "# 7.1 Team strength index (Elo-like) - Re-run based on cleaned matches\n",
    "print(\"\\n7.1 Team strength index (Elo-like)\")\n",
    "team_ratings = {team: 1500 for team in pd.concat([matches_clean['team1'], matches_clean['team2']]).dropna().unique()}\n",
    "K = 32\n",
    "\n",
    "def update_ratings(winner, loser, ratings, k=K):\n",
    "    if winner not in ratings or loser not in ratings: return # Skip if team not initialized\n",
    "    expected_winner = 1 / (1 + 10 ** ((ratings[loser] - ratings[winner]) / 400))\n",
    "    expected_loser = 1 / (1 + 10 ** ((ratings[winner] - ratings[loser]) / 400))\n",
    "    ratings[winner] += k * (1 - expected_winner)\n",
    "    ratings[loser] += k * (0 - expected_loser)\n",
    "\n",
    "# Iterate through sorted matches to update ratings chronologically\n",
    "temp_ratings = team_ratings.copy()\n",
    "match_ratings = [] # Store ratings *before* each match\n",
    "for idx, row in matches_clean.sort_values('date').iterrows():\n",
    "     # Store ratings *before* this match happens\n",
    "    match_ratings.append({\n",
    "        'match_id': row['id'],\n",
    "        f\"{row['team1']}_rating\": temp_ratings.get(row['team1'], 1500),\n",
    "        f\"{row['team2']}_rating\": temp_ratings.get(row['team2'], 1500)\n",
    "    })\n",
    "    # Update ratings after the match\n",
    "    if row['winner'] not in ['No Result', 'Unknown', None] and pd.notna(row['winner']):\n",
    "        if row['winner'] == row['team1']:\n",
    "            update_ratings(row['team1'], row['team2'], temp_ratings)\n",
    "        elif row['winner'] == row['team2']:\n",
    "             update_ratings(row['team2'], row['team1'], temp_ratings)\n",
    "\n",
    "final_team_ratings = temp_ratings\n",
    "sorted_ratings = sorted(final_team_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Final Team ratings:\")\n",
    "for team, rating in sorted_ratings:\n",
    "    print(f\"{team}: {rating:.2f}\")\n",
    "\n",
    "\n",
    "# 7.2 Interaction features\n",
    "print(\"\\n7.2 Interaction features\")\n",
    "# Example: Toss decision * toss winner is home team\n",
    "matches_clean['toss_winner_is_home'] = np.where(\n",
    "    matches_clean['toss_winner'] == matches_clean['team1'], matches_clean['is_team1_home'],\n",
    "    np.where(matches_clean['toss_winner'] == matches_clean['team2'], matches_clean['is_team2_home'], 0)\n",
    ")\n",
    "matches_clean['toss_field_home_win'] = (matches_clean['toss_decision'] == 'field') & (matches_clean['toss_winner_is_home'] == 1) & (matches_clean['toss_winner_won_match'] == 1)\n",
    "matches_clean['toss_bat_home_win'] = (matches_clean['toss_decision'] == 'bat') & (matches_clean['toss_winner_is_home'] == 1) & (matches_clean['toss_winner_won_match'] == 1)\n",
    "print(f\"Prob win | Field, Home, Won Toss: {matches_clean['toss_field_home_win'].mean():.3f}\")\n",
    "print(f\"Prob win | Bat, Home, Won Toss: {matches_clean['toss_bat_home_win'].mean():.3f}\")\n",
    "\n",
    "# 7.3 Team rivalry analysis\n",
    "print(\"\\n7.3 Team rivalry analysis\")\n",
    "def analyze_head_to_head(team1, team2, df):\n",
    "    matches = df[((df['team1'] == team1) & (df['team2'] == team2)) |\n",
    "                 ((df['team1'] == team2) & (df['team2'] == team1))].copy()\n",
    "    valid_matches = matches[~matches['winner'].isin(['No Result', 'Unknown'])]\n",
    "    total_matches = len(valid_matches)\n",
    "    if total_matches == 0:\n",
    "        return {'total_matches': 0, f'{team1}_wins': 0, f'{team2}_wins': 0, f'{team1}_win_rate': 0, f'{team2}_win_rate': 0}\n",
    "\n",
    "    team1_wins = len(valid_matches[valid_matches['winner'] == team1])\n",
    "    team2_wins = len(valid_matches[valid_matches['winner'] == team2])\n",
    "\n",
    "    return {\n",
    "        'total_matches': total_matches,\n",
    "        f'{team1}_wins': team1_wins, f'{team2}_wins': team2_wins,\n",
    "        f'{team1}_win_rate': team1_wins / total_matches,\n",
    "        f'{team2}_win_rate': team2_wins / total_matches\n",
    "    }\n",
    "\n",
    "mi_csk_rivalry = analyze_head_to_head('Mumbai Indians', 'Chennai Super Kings', matches_clean)\n",
    "print(\"Mumbai Indians vs Chennai Super Kings Head-to-Head:\")\n",
    "print(mi_csk_rivalry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af03032",
   "metadata": {
    "papermill": {
     "duration": 0.081079,
     "end_time": "2025-04-03T08:45:24.304389",
     "exception": false,
     "start_time": "2025-04-03T08:45:24.223310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Modeling Approaches (Demo)\n",
    "#### Outline potential modeling strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed404d",
   "metadata": {
    "papermill": {
     "duration": 0.080135,
     "end_time": "2025-04-03T08:45:24.466902",
     "exception": false,
     "start_time": "2025-04-03T08:45:24.386767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8.1 Classification Models (Predicting Winner)\n",
    "#### Training and evaluating a Random Forest model to predict if Team 1 wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8685dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:24.631438Z",
     "iopub.status.busy": "2025-04-03T08:45:24.631104Z",
     "iopub.status.idle": "2025-04-03T08:45:26.660883Z",
     "shell.execute_reply": "2025-04-03T08:45:26.659711Z"
    },
    "papermill": {
     "duration": 2.114952,
     "end_time": "2025-04-03T08:45:26.663465",
     "exception": false,
     "start_time": "2025-04-03T08:45:24.548513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n8.1 Classification Models (Training & Evaluating Random Forest for Team 1 Win)\")\n",
    "\n",
    "# Import necessary evaluation metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             roc_auc_score, roc_curve, ConfusionMatrixDisplay)\n",
    "\n",
    "# Check if training/testing data is available from step 6.3\n",
    "if 'X_train' in locals() and X_train is not None and \\\n",
    "   'y_train' in locals() and y_train is not None and \\\n",
    "   'X_test' in locals() and X_test is not None and \\\n",
    "   'y_test' in locals() and y_test is not None:\n",
    "\n",
    "    print(f\"Using training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "\n",
    "    # --- Define the Model Pipeline ---\n",
    "    # Pipeline helps ensure steps are applied consistently\n",
    "    pipeline_rf = Pipeline([\n",
    "        # Include scaling here if it wasn't applied universally to X before splitting\n",
    "        # ('scaler', StandardScaler()), # Uncomment if X_train/X_test are not scaled yet\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=150,          # Increased estimators slightly\n",
    "            class_weight='balanced',   # Good for potentially imbalanced win rates\n",
    "            max_depth=15,              # Limit tree depth to prevent overfitting\n",
    "            min_samples_split=10,      # Minimum samples to split a node\n",
    "            min_samples_leaf=5,        # Minimum samples at a leaf node\n",
    "            n_jobs=-1                  # Use all available CPU cores\n",
    "            ))\n",
    "    ])\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    print(\"\\nTraining the Random Forest model...\")\n",
    "    try:\n",
    "        pipeline_rf.fit(X_train, y_train)\n",
    "        print(\"Model training completed.\")\n",
    "\n",
    "        # --- Make Predictions ---\n",
    "        print(\"\\nMaking predictions on the test set...\")\n",
    "        y_pred = pipeline_rf.predict(X_test)\n",
    "        y_pred_proba = pipeline_rf.predict_proba(X_test)[:, 1] # Probability of Team 1 winning (class 1)\n",
    "        print(\"Predictions generated.\")\n",
    "\n",
    "        # --- Evaluate the Model ---\n",
    "        print(\"\\n--- Model Evaluation ---\")\n",
    "\n",
    "        # 1. Accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # 2. AUC-ROC Score\n",
    "        try:\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not calculate AUC-ROC: {e}\") # Handle cases with only one class in y_test\n",
    "\n",
    "        # 3. Classification Report (Precision, Recall, F1-Score)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        # Use target_names for better readability if desired\n",
    "        print(classification_report(y_test, y_pred, target_names=['Team 2 Wins/Other', 'Team 1 Wins']))\n",
    "\n",
    "        # 4. Confusion Matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        # Plotting the Confusion Matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Team 2 Wins/Other', 'Team 1 Wins'])\n",
    "        fig, ax = plt.subplots(figsize=(6, 5)) # Create figure and axes explicitly\n",
    "        disp.plot(cmap='Blues', ax=ax) # Pass axes to display\n",
    "        plt.title('Confusion Matrix - Team 1 Win Prediction')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # 5. ROC Curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Chance')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # --- Feature Importance (for Random Forest) ---\n",
    "        print(\"\\n--- Feature Importance ---\")\n",
    "        # Access the classifier step in the pipeline\n",
    "        if isinstance(pipeline_rf.named_steps['classifier'], RandomForestClassifier):\n",
    "            classifier = pipeline_rf.named_steps['classifier']\n",
    "            importances = classifier.feature_importances_\n",
    "            feature_names = X_train.columns # Get feature names from training data\n",
    "\n",
    "            # Create a DataFrame for better visualization\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "            # Display top N features\n",
    "            n_top_features = 20\n",
    "            print(f\"\\nTop {n_top_features} Features:\")\n",
    "            print(feature_importance_df.head(n_top_features))\n",
    "\n",
    "            # Plot top N features\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.barplot(x='importance', y='feature', data=feature_importance_df.head(n_top_features), palette='viridis')\n",
    "            plt.title(f'Top {n_top_features} Feature Importances - Random Forest')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Could not extract feature importances (model is not a RandomForestClassifier or pipeline structure differs).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during model training or evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Classification Model Training & Evaluation because training/testing data (X_train, y_train, X_test, y_test) is not available or not properly defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14ed1c",
   "metadata": {
    "papermill": {
     "duration": 0.085697,
     "end_time": "2025-04-03T08:45:26.836690",
     "exception": false,
     "start_time": "2025-04-03T08:45:26.750993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8.2 Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb33359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:27.010932Z",
     "iopub.status.busy": "2025-04-03T08:45:27.010552Z",
     "iopub.status.idle": "2025-04-03T08:45:27.957012Z",
     "shell.execute_reply": "2025-04-03T08:45:27.955376Z"
    },
    "papermill": {
     "duration": 1.037644,
     "end_time": "2025-04-03T08:45:27.961522",
     "exception": false,
     "start_time": "2025-04-03T08:45:26.923878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 8.2 Regression Models ---\n",
    "print(\"\\n8.2 Regression Models (Training & Evaluating GBR for Run Margin)\")\n",
    "\n",
    "# Import necessary evaluation metrics for regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler # Ensure scaler is imported if needed in pipeline\n",
    "\n",
    "# --- Data Preparation for Regression ---\n",
    "print(\"\\nPreparing data for run margin prediction...\")\n",
    "\n",
    "# Define the target population: Matches won by runs\n",
    "# Ensure 'matches_clean' and 'model_df_encoded' are available\n",
    "if 'matches_clean' in locals() and 'model_df_encoded' in locals():\n",
    "    reg_target_df = matches_clean[matches_clean['win_by_runs'] > 0].copy()\n",
    "\n",
    "    if not reg_target_df.empty:\n",
    "        print(f\"Found {len(reg_target_df)} matches won by runs.\")\n",
    "        # Align features from model_df_encoded with this subset using index\n",
    "        # Ensure model_df_encoded has been created in step 6.1\n",
    "        if reg_target_df.index.isin(model_df_encoded.index).all():\n",
    "            X_reg_all = model_df_encoded.loc[reg_target_df.index].copy()\n",
    "\n",
    "            # Define target variable\n",
    "            y_reg = reg_target_df['win_by_runs']\n",
    "\n",
    "            # Define features: Drop target and other potential leakage columns\n",
    "            features_to_drop = ['win_by_runs', 'win_by_wickets'] # Always drop target and other margin\n",
    "            # Optionally drop others if they leak info strongly for this specific task\n",
    "            # features_to_drop.append('toss_winner_won_match')\n",
    "\n",
    "            cols_exist_to_drop = [col for col in features_to_drop if col in X_reg_all.columns]\n",
    "            X_reg = X_reg_all.drop(columns=cols_exist_to_drop)\n",
    "            print(f\"Shape of feature set for regression (X_reg): {X_reg.shape}\")\n",
    "            print(f\"Shape of target set for regression (y_reg): {y_reg.shape}\")\n",
    "\n",
    "            # Check alignment one last time\n",
    "            if len(X_reg) == len(y_reg):\n",
    "                # --- Train-Test Split for Regression Data ---\n",
    "                X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "                    X_reg, y_reg, test_size=0.25, random_state=42\n",
    "                )\n",
    "                print(f\"\\nRegression data split:\")\n",
    "                print(f\"Training set shape: X={X_reg_train.shape}, y={y_reg_train.shape}\")\n",
    "                print(f\"Test set shape: X={X_reg_test.shape}, y={y_reg_test.shape}\")\n",
    "\n",
    "                # --- Define the Model Pipeline (with Scaling) ---\n",
    "                pipeline_gbr = Pipeline([\n",
    "                    ('scaler', StandardScaler()), # Scaling is generally important for GBR\n",
    "                    ('regressor', GradientBoostingRegressor(\n",
    "                        random_state=42,\n",
    "                        n_estimators=100,         # Number of boosting stages\n",
    "                        learning_rate=0.1,        # Step size shrinkage\n",
    "                        max_depth=5,              # Limit tree depth\n",
    "                        min_samples_split=10,     # Min samples to split\n",
    "                        min_samples_leaf=5,       # Min samples per leaf\n",
    "                        loss='squared_error'      # Default loss function for regression\n",
    "                    ))\n",
    "                ])\n",
    "\n",
    "                # --- Train the Model ---\n",
    "                print(\"\\nTraining the Gradient Boosting Regressor model...\")\n",
    "                try:\n",
    "                    pipeline_gbr.fit(X_reg_train, y_reg_train)\n",
    "                    print(\"Model training completed.\")\n",
    "\n",
    "                    # --- Make Predictions ---\n",
    "                    print(\"\\nMaking predictions on the test set...\")\n",
    "                    y_reg_pred = pipeline_gbr.predict(X_reg_test)\n",
    "                    print(\"Predictions generated.\")\n",
    "\n",
    "                    # --- Evaluate the Model ---\n",
    "                    print(\"\\n--- Regression Model Evaluation ---\")\n",
    "\n",
    "                    # 1. Mean Absolute Error (MAE)\n",
    "                    mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "                    print(f\"Mean Absolute Error (MAE): {mae:.2f} runs\")\n",
    "\n",
    "                    # 2. Root Mean Squared Error (RMSE)\n",
    "                    rmse = mean_squared_error(y_reg_test, y_reg_pred, squared=False)\n",
    "                    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} runs\")\n",
    "\n",
    "                    # 3. R-squared (R²) Score\n",
    "                    r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "                    print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "                    # 4. Scatter Plot: Actual vs Predicted\n",
    "                    plt.figure(figsize=(8, 8))\n",
    "                    plt.scatter(y_reg_test, y_reg_pred, alpha=0.5)\n",
    "                    plt.plot([y_reg_test.min(), y_reg_test.max()], [y_reg_test.min(), y_reg_test.max()], '--r', lw=2, label='Perfect Prediction')\n",
    "                    plt.xlabel(\"Actual Win Margin (Runs)\")\n",
    "                    plt.ylabel(\"Predicted Win Margin (Runs)\")\n",
    "                    plt.title(\"Actual vs. Predicted Win Margin (Runs) - GBR\")\n",
    "                    plt.legend()\n",
    "                    plt.grid(True)\n",
    "                    plt.show()\n",
    "\n",
    "                    # --- Feature Importance (for Gradient Boosting) ---\n",
    "                    print(\"\\n--- Feature Importance (GBR) ---\")\n",
    "                    # Access the regressor step in the pipeline\n",
    "                    if isinstance(pipeline_gbr.named_steps['regressor'], GradientBoostingRegressor):\n",
    "                        regressor = pipeline_gbr.named_steps['regressor']\n",
    "                        importances = regressor.feature_importances_\n",
    "                        feature_names = X_reg_train.columns # Get feature names from training data\n",
    "\n",
    "                        # Create a DataFrame\n",
    "                        feature_importance_df_reg = pd.DataFrame({\n",
    "                            'feature': feature_names,\n",
    "                            'importance': importances\n",
    "                        }).sort_values('importance', ascending=False)\n",
    "\n",
    "                        # Display top N features\n",
    "                        n_top_features = 20\n",
    "                        print(f\"\\nTop {n_top_features} Features:\")\n",
    "                        print(feature_importance_df_reg.head(n_top_features))\n",
    "\n",
    "                        # Plot top N features\n",
    "                        plt.figure(figsize=(10, 8))\n",
    "                        sns.barplot(x='importance', y='feature', data=feature_importance_df_reg.head(n_top_features), palette='viridis')\n",
    "                        plt.title(f'Top {n_top_features} Feature Importances - GBR (Run Margin Prediction)')\n",
    "                        plt.xlabel('Importance Score')\n",
    "                        plt.ylabel('Feature')\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        print(\"Could not extract feature importances (model is not GradientBoostingRegressor or pipeline structure differs).\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nAn error occurred during regression model training or evaluation: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc() # Print detailed traceback\n",
    "\n",
    "            else:\n",
    "                print(\"\\nSkipping Regression: Feature and target data lengths do not match after alignment.\")\n",
    "        else:\n",
    "            print(\"\\nSkipping Regression: Index mismatch between target data and encoded features.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Regression: No matches found meeting the criteria (win_by_runs > 0).\")\n",
    "else:\n",
    "    print(\"\\nSkipping Regression Model Training & Evaluation because base dataframes ('matches_clean' or 'model_df_encoded') are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e34ea3",
   "metadata": {},
   "source": [
    "# 8.3 Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. R-squared (R²) Score\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "# Optional: Custom Regression Accuracy (based on tolerance)\n",
    "tolerance = 10  # runs\n",
    "correct = (abs(y_reg_test - y_reg_pred) <= tolerance).sum()\n",
    "total = len(y_reg_test)\n",
    "regression_accuracy = correct / total\n",
    "\n",
    "print(f\"Custom Regression Accuracy (±{tolerance} runs): {regression_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d789b",
   "metadata": {
    "papermill": {
     "duration": 0.095344,
     "end_time": "2025-04-03T08:45:28.161155",
     "exception": false,
     "start_time": "2025-04-03T08:45:28.065811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8.4 Time Series Analysis (Runs per Over and Seasonal Trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213f70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:28.357012Z",
     "iopub.status.busy": "2025-04-03T08:45:28.356572Z",
     "iopub.status.idle": "2025-04-03T08:45:28.753117Z",
     "shell.execute_reply": "2025-04-03T08:45:28.752152Z"
    },
    "papermill": {
     "duration": 0.498445,
     "end_time": "2025-04-03T08:45:28.756248",
     "exception": false,
     "start_time": "2025-04-03T08:45:28.257803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 8.3 Time Series Analysis ---\n",
    "print(\"\\n8.3 Time Series Analysis (Runs per Over and Seasonal Trends)\")\n",
    "\n",
    "# Ensure 'deliveries_merged' is available from Section 4b\n",
    "if 'deliveries_merged' in locals() and not deliveries_merged.empty:\n",
    "\n",
    "    # --- Analysis 1: Average Runs per Over (Overall and by Innings) ---\n",
    "    print(\"\\nAnalyzing average runs per over...\")\n",
    "    # Group by match, inning, and over to get total runs in that specific over\n",
    "    runs_per_over_inning = deliveries_merged.groupby(['match_id', 'inning', 'over'])['total_runs'].sum().reset_index()\n",
    "\n",
    "    # Calculate overall average runs for each over number (0-19)\n",
    "    avg_runs_per_over_overall = runs_per_over_inning.groupby('over')['total_runs'].mean()\n",
    "\n",
    "    # Calculate average runs separately for first and second innings\n",
    "    avg_runs_per_over_inning1 = runs_per_over_inning[runs_per_over_inning['inning'] == 1].groupby('over')['total_runs'].mean()\n",
    "    avg_runs_per_over_inning2 = runs_per_over_inning[runs_per_over_inning['inning'] == 2].groupby('over')['total_runs'].mean()\n",
    "\n",
    "    # Create a combined plot\n",
    "    fig_ts_innings = go.Figure()\n",
    "\n",
    "    # Add Overall Trace\n",
    "    if not avg_runs_per_over_overall.empty:\n",
    "        fig_ts_innings.add_trace(go.Scatter(\n",
    "            x=avg_runs_per_over_overall.index,\n",
    "            y=avg_runs_per_over_overall.values,\n",
    "            mode='lines+markers', name='Overall Average', line=dict(color='grey', dash='dash')\n",
    "        ))\n",
    "\n",
    "    # Add Inning 1 Trace\n",
    "    if not avg_runs_per_over_inning1.empty:\n",
    "        fig_ts_innings.add_trace(go.Scatter(\n",
    "            x=avg_runs_per_over_inning1.index,\n",
    "            y=avg_runs_per_over_inning1.values,\n",
    "            mode='lines+markers', name='Inning 1 Average', line=dict(color='royalblue')\n",
    "        ))\n",
    "\n",
    "    # Add Inning 2 Trace\n",
    "    if not avg_runs_per_over_inning2.empty:\n",
    "        fig_ts_innings.add_trace(go.Scatter(\n",
    "            x=avg_runs_per_over_inning2.index,\n",
    "            y=avg_runs_per_over_inning2.values,\n",
    "            mode='lines+markers', name='Inning 2 Average', line=dict(color='firebrick')\n",
    "        ))\n",
    "\n",
    "    # Update layout for clarity\n",
    "    fig_ts_innings.update_layout(\n",
    "        title='Average Runs Scored per Over (Overall vs. Innings)',\n",
    "        xaxis_title='Over Number (0-indexed)',\n",
    "        yaxis_title='Average Runs Scored',\n",
    "        xaxis=dict(tickmode='linear'), # Ensure x-axis treats over number numerically\n",
    "        legend_title_text='Metric'\n",
    "    )\n",
    "    fig_ts_innings.show()\n",
    "\n",
    "\n",
    "    # --- Analysis 2: Trend of Runs in Key Phases Over Seasons ---\n",
    "    # Requires 'season_year' from the merge\n",
    "    if 'season_year' in deliveries_merged.columns:\n",
    "        print(\"\\nAnalyzing run rate trends in key phases over seasons...\")\n",
    "\n",
    "        # Define phases (example: Powerplay 0-5, Middle 6-14, Death 15-19)\n",
    "        deliveries_merged['phase'] = pd.cut(\n",
    "            deliveries_merged['over'],\n",
    "            bins=[-1, 5, 14, 19],          # Bins: (-1, 5], (5, 14], (14, 19]\n",
    "            labels=['Powerplay', 'Middle', 'Death']\n",
    "        )\n",
    "\n",
    "        # Calculate average runs per ball (run rate) in each phase per season\n",
    "        # Group by season, phase, and inning first\n",
    "        phase_data = deliveries_merged.groupby(['season_year', 'phase', 'inning'])['total_runs'].agg(['sum', 'count']).reset_index()\n",
    "        # Calculate runs per ball (count includes all deliveries in that phase)\n",
    "        phase_data['runs_per_ball'] = phase_data['sum'] / phase_data['count']\n",
    "        phase_data['run_rate_per_over'] = phase_data['runs_per_ball'] * 6 # Approximate run rate\n",
    "\n",
    "        # Plot the trend for each phase (separating innings)\n",
    "        fig_phase_trend = px.line(\n",
    "            phase_data.sort_values(['season_year', 'inning', 'phase']),\n",
    "            x='season_year',\n",
    "            y='run_rate_per_over',\n",
    "            color='phase',          # Color lines by phase\n",
    "            line_dash='inning',     # Dash lines by inning (requires inning as string/category)\n",
    "            markers=True,\n",
    "            facet_col='inning',     # Optional: Separate plots for each inning\n",
    "            labels={'season_year': 'Season', 'run_rate_per_over': 'Approx Run Rate per Over', 'phase':'Phase', 'inning':'Inning'},\n",
    "            title='Run Rate Trend in Different Phases Over Seasons (by Inning)'\n",
    "        )\n",
    "        fig_phase_trend.update_layout(xaxis={'type': 'category'}) # Treat year as category\n",
    "        fig_phase_trend.for_each_annotation(lambda a: a.update(text=f\"Inning {a.text.split('=')[-1]}\")) # Clean up facet titles\n",
    "        fig_phase_trend.show()\n",
    "\n",
    "    else:\n",
    "        print(\"\\nSkipping seasonal trend analysis as 'season_year' is not available in deliveries data.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Time Series Analysis as 'deliveries_merged' data is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd6036",
   "metadata": {
    "papermill": {
     "duration": 0.106739,
     "end_time": "2025-04-03T08:45:28.969131",
     "exception": false,
     "start_time": "2025-04-03T08:45:28.862392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Visualisations for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c0296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:29.175384Z",
     "iopub.status.busy": "2025-04-03T08:45:29.174978Z",
     "iopub.status.idle": "2025-04-03T08:45:30.518911Z",
     "shell.execute_reply": "2025-04-03T08:45:30.517655Z"
    },
    "papermill": {
     "duration": 1.454867,
     "end_time": "2025-04-03T08:45:30.526009",
     "exception": false,
     "start_time": "2025-04-03T08:45:29.071142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n---------- 9. VISUALIZATIONS FOR Dashboard ----------\")\n",
    "print(\"Generating selected presentation-ready visualizations...\")\n",
    "\n",
    "# 9.1 Interactive Batsman Comparison (Example: Runs vs Strike Rate)\n",
    "batsman_stats = pd.DataFrame({\n",
    "    'runs': batsman_runs,\n",
    "    'balls': balls_faced,\n",
    "    'strike_rate': batsman_strike_rate,\n",
    "    'average': batsman_average,\n",
    "    'dismissals': dismissals\n",
    "}).dropna(subset=['strike_rate', 'average']) # Drop if SR or Avg couldn't be calculated\n",
    "\n",
    "# Filter for plotting (e.g., min 500 runs)\n",
    "batsman_stats_filtered = batsman_stats[batsman_stats['runs'] >= 1000].reset_index()\n",
    "\n",
    "fig_scatter = px.scatter(batsman_stats_filtered, x='strike_rate', y='average',\n",
    "                         size='runs', color='runs', hover_name='batter',\n",
    "                         title='Batsman Performance Comparison (Average vs Strike Rate, Size=Runs, Min 1000 Runs)',\n",
    "                         labels={'strike_rate': 'Strike Rate', 'average': 'Average Runs per Dismissal', 'runs': 'Total Runs'},\n",
    "                         color_continuous_scale=px.colors.sequential.Viridis)\n",
    "fig_scatter.update_layout(xaxis_title=\"Strike Rate\", yaxis_title=\"Average\")\n",
    "fig_scatter.show()\n",
    "\n",
    "# Example Placeholder: Heatmap of Team vs Venue Win Rates\n",
    "venue_pivot = venue_performance[venue_performance['matches'] >= 5].pivot(index='team', columns='venue', values='win_rate')\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(venue_pivot, annot=True, fmt=\".1%\", cmap=\"viridis\", linewidths=.5)\n",
    "plt.title('Team Win Rate (%) at Different Venues (Min 5 Matches Played)')\n",
    "plt.xlabel('Venue')\n",
    "plt.ylabel('Team')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da36234",
   "metadata": {
    "papermill": {
     "duration": 0.120407,
     "end_time": "2025-04-03T08:45:30.771106",
     "exception": false,
     "start_time": "2025-04-03T08:45:30.650699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Summary of Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744d3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:45:31.072157Z",
     "iopub.status.busy": "2025-04-03T08:45:31.071721Z",
     "iopub.status.idle": "2025-04-03T08:45:31.572706Z",
     "shell.execute_reply": "2025-04-03T08:45:31.571321Z"
    },
    "papermill": {
     "duration": 0.621721,
     "end_time": "2025-04-03T08:45:31.574795",
     "exception": false,
     "start_time": "2025-04-03T08:45:30.953074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 10. SUMMARY OF ANALYSIS ---\n",
    "print(\"\\n---------- 10. SUMMARY OF ANALYSIS ----------\")\n",
    "print(\"\"\"\n",
    "Key insights from the analysis using matches.csv and deliveries.csv:\n",
    "1.  Data Cleaning: Successfully adapted cleaning steps for the new datasets, handling missing values and deriving essential columns like 'win_by_runs'/'win_by_wickets'. Standardized team names.\n",
    "2.  EDA (Matches): Analyzed seasonal trends, team dominance, toss impact, venue preferences, and player awards based on the match-level data.\n",
    "3.  EDA (Deliveries): Performed detailed batsman analysis, identifying top run-scorers, best strike rates, highest averages, and most sixes overall and per season.\n",
    "4.  Feature Engineering: Created matchup features, team win rates, simplified home advantage, and seasonal phase indicators from match data. Explored Elo ratings.\n",
    "5.  Modeling Prep: Prepared data for modeling by encoding categorical features and scaling numerical ones, demonstrating setup for classification and regression tasks.\n",
    "6.  Visualization: Utilized Plotly for interactive visualizations of match trends, team performance, venue stats, and detailed batsman statistics (runs, SR, Avg, Sixes).\n",
    "\n",
    "This analysis provides a more comprehensive view by combining match outcomes with ball-by-ball details, enabling deeper player performance insights alongside match-level trends.\n",
    "\"\"\")\n",
    "\n",
    "# --- 11. APPLIED STATISTICS ---\n",
    "print(\"\\n---------- 11. APPLIED STATISTICS ----------\")\n",
    "\n",
    "# 11.1 Hypothesis Testing (Toss Advantage - Rerun on cleaned data)\n",
    "print(\"\\n11.1 Hypothesis Testing (Toss Advantage)\")\n",
    "from scipy import stats\n",
    "\n",
    "# Use the previously calculated 'valid_matches_for_toss_impact' which excludes 'No Result' etc.\n",
    "toss_winner_won = valid_matches_for_toss_impact['toss_winner_won_match']\n",
    "observed_p = toss_winner_won.mean()\n",
    "n_trials = len(toss_winner_won)\n",
    "n_successes = toss_winner_won.sum()\n",
    "\n",
    "if n_trials > 0:\n",
    "    result = stats.binomtest(n_successes, n_trials, p=0.5, alternative='greater') # Test if significantly > 50%\n",
    "    print(f\"Observed proportion of toss winners winning the match: {observed_p:.4f}\")\n",
    "    print(f\"Binomial test p-value (one-sided, > 0.5): {result.pvalue:.4f}\")\n",
    "    if result.pvalue < 0.05:\n",
    "        print(\"Reject null hypothesis: Winning the toss provides a statistically significant advantage.\")\n",
    "    else:\n",
    "        print(\"Fail to reject null hypothesis: No significant evidence that winning the toss provides an advantage (p >= 0.05).\")\n",
    "else:\n",
    "    print(\"Not enough valid matches to perform hypothesis test.\")\n",
    "\n",
    "\n",
    "# 11.2 Statistical Correlations (Matches Data)\n",
    "print(\"\\n11.2 Statistical Correlations\")\n",
    "# Select a subset of numerical/binary features from matches_clean\n",
    "correlation_features = ['season_year', 'win_by_runs', 'win_by_wickets',\n",
    "                        'toss_winner_won_match', 'dl_applied', 'super_over',\n",
    "                        'is_team1_home', 'is_team2_home']\n",
    "# Ensure columns exist before calculating correlation\n",
    "valid_corr_features = [col for col in correlation_features if col in matches_clean.columns]\n",
    "correlation_matrix = matches_clean[valid_corr_features].corr()\n",
    "\n",
    "print(\"Correlation matrix (Matches Data):\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize Correlation Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Key Match Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nEnd of IPL Data Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ecfd7",
   "metadata": {},
   "source": [
    "# 11. Creating GUI using tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Step 1: Label Encoding for GUI Inputs ---\n",
    "label_encoders = {}\n",
    "dropdown_options = {}\n",
    "\n",
    "# Use matches_clean to get actual categorical values\n",
    "for col in ['team1', 'team2', 'toss_winner']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(matches_clean[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    dropdown_options[col] = list(le.classes_)\n",
    "\n",
    "# --- Step 2: Build GUI ---\n",
    "root = tk.Tk()\n",
    "root.title(\"IPL Win Margin Predictor (GBR)\")\n",
    "root.geometry(\"400x350\")\n",
    "\n",
    "# Input Fields with Dropdowns\n",
    "input_fields = {\n",
    "    'team1': tk.StringVar(),\n",
    "    'team2': tk.StringVar(),\n",
    "    'toss_winner': tk.StringVar()\n",
    "}\n",
    "\n",
    "# GUI Widgets\n",
    "row = 0\n",
    "for field in input_fields:\n",
    "    ttk.Label(root, text=field.replace('_', ' ').title()).grid(column=0, row=row, padx=10, pady=10, sticky='w')\n",
    "    dropdown = ttk.Combobox(root, textvariable=input_fields[field], values=dropdown_options[field], state='readonly', width=25)\n",
    "    dropdown.grid(column=1, row=row, padx=10, pady=10)\n",
    "    dropdown.current(0)  # Set default selection\n",
    "    row += 1\n",
    "\n",
    "# Result Display Label\n",
    "result_var = tk.StringVar()\n",
    "ttk.Label(root, textvariable=result_var, font=('Arial', 12, 'bold')).grid(column=0, row=row+1, columnspan=2, pady=20)\n",
    "\n",
    "# --- Step 3: Predict Function ---\n",
    "def predict_margin():\n",
    "    try:\n",
    "        # Encode user input\n",
    "        encoded_input = {\n",
    "            col: label_encoders[col].transform([input_fields[col].get()])[0]\n",
    "            for col in input_fields\n",
    "        }\n",
    "\n",
    "        # Build input vector for model\n",
    "        input_array = np.zeros((1, X_reg_train.shape[1]))\n",
    "        for i, col in enumerate(X_reg_train.columns):\n",
    "            if col in encoded_input:\n",
    "                input_array[0, i] = encoded_input[col]\n",
    "\n",
    "        # Prediction\n",
    "        prediction = pipeline_gbr.predict(input_array)[0]\n",
    "        team1_name = input_fields['team1'].get()\n",
    "        result_var.set(f\"{team1_name} is predicted to win by {prediction:.2f} runs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        result_var.set(f\"Error: {e}\")\n",
    "\n",
    "# Predict Button\n",
    "ttk.Button(root, text=\"Predict\", command=predict_margin).grid(column=0, row=row, columnspan=2, pady=10)\n",
    "\n",
    "# Run GUI\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 990900,
     "sourceId": 8637500,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 71.490168,
   "end_time": "2025-04-03T08:45:32.862524",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-03T08:44:21.372356",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
